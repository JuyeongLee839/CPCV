{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample Task2에서 구현하고자 하는 모델처럼 값을 추정하는 Deterministic Prediction Model의 한계와 금융 시계열이 갖고 있는 여러가지 문제점들은 크래프트에서 공개한 영상들에서 너무나 잘 설명해 주고 있습니다.\n",
    ">- 오버피팅\n",
    ">- Low siganal to noise ratio \n",
    ">- 짧은 시계열길이\n",
    "\n",
    "저는 이 내용들 이외에 그동안 공부하고 크래프트 영상을 보며 고민한 지점들 중 sample task2와 관련한 부분에 대해서 적어보려 합니다. 예전 영상들을 보고 생각한 부분이다 보니 지금은 이미 연구해 보셨을 수도 있지만,제가 하던 공부들을 어떻게 하면 크래프트의 연구에 적용할 수 있을까 고민해본 지점들이라 생각해 주시면 감사하겠습니다<br><br>\n",
    " \n",
    "### 1. 금융데이터셋에서의 Data Split 문제와 Cross Validation\n",
    "테크톡에서 시간 순서대로 시계열을 split한 뒤, 병렬적으로 학습시켜 경쟁을 통해 오버피팅을 방지하는 방법론에 대해서 인상 깊게 들었습니다. 하지만 같은 Validation set만으로 오버피팅을 체크하다보면 특정 시기의 데이터에 대해 bias가 생기지 않을까? 생각을 한 적이 있습니다. 물론 이 문제를 해결하고자 기존 머신러닝에서 사용하는 Cross Validation을 그대로 사용한다면, 당연히 look ahead bias와 overfitting이 발생하게 됩니다. Prado는 Training Set과 Validation Set 사이에 information leakage가 발생하지 않도록 Purging과 Embargo를 통해서 데이터 셋을 나누는 방법을 제안했습니다. 또한 테스트 셋을 한 번 더 나눔으로써, CV의 path 자체도 여러가지로 만들 수 있는 combinational purged cross validation 방법론도 제안했습니다. Prado가 제안하는 CV 방법론과, 병렬적으로 학습시키는 방법을 조합한다면 이론적으로 더욱 촘촘한 모델이 될 것이라 생각합니다. Purging Embargo는 조건2를 구현하며 코드를 정리해 두었습니다. 보다 자세한 내용은 AFML 7장과 12장 내용을 참고하시면 될 것 같습니다.\n",
    "\n",
    "### 2. 수익률을 사용한 예측의 문제 : 금융시계열의 정상성과 예측력\n",
    "Denoising Autoencoder 부분에서 설명하셨듯 금융시계열은 Low signal to noise ratio를 갖고 있습니다. 게다가 non stationarity를 해결하기 위해 차분한 시계열을 학습에 사용하곤 하는데, Stationarity와 Memory는 trade-off 관계에 있으므로 차분한 시계열은 memory가 사라지게 됩니다. 특히 조건 3번에서처럼 Price Series를 1차로 차분(log diff)한 수익률데이터에는 메모리가 거의 남아있지 않습니다. 정상성이 미래 예측의 전제조건이기는 하나, 비정상성 정도에 비해 과도하게 차분된다면 오히려 예측 성능을 저하시키는 결과를 가져옵니다. 따라서 Price Series를 1 이하의 숫자로 차분하는 Fractional difference를 사용하면, 정상성을 달성하는 동시에 메모리는 최대한 보존할 수 있습니다. 1 이하의 숫자로 차분된 시계열을 사용하여 feature extraction을 한다면 성능 향상에 도움이 되지 않을까 생각했습니다. \n",
    ">Alexander [2001] Return transformations make the series stationary at the expense of removing all memory from the original series. Although stationarity is a necessary property for inferential purposes, it is rarely the case in signal processing that we wish all memory to be erased, as that memory is the basis for the model’s predictive power \n",
    "\n",
    "### 3. 시간단위의 Data structure가 갖는 문제\n",
    "이번 과제와는 약간 동 떨어져 있지만, Data structure를 문제에서 주어진 월별 수익률 같이 시간단위가 아닌, 거래대금 단위로 묶어서 분석을 진행한다면 주식 단위에서 추가로 알파를 찾을 수 있지 않을까 생각합니다. 시장에 정보는 동일한 시간 간격으로 오지 않기에, 시간 단위의 데이터는 거래량이 많은 시기의 singal은 under sample하고 거래량이 적을 때의 정보는 oversample하게 됩니다. 거래량이나 특히 거래대금으로 단위를 구분할 경우 기업의 액면분할과 같은 이슈에도 robust하고 통계적으로 좀 더 성질이 좋은 성질(iid, no serial correlation)을 띄게 됩니다 이를 기존의 크래프트의 연구에 접목할 수 있는 부분을 찾고 싶습니다.\n",
    ">Ane and Geman [2000] sampling as a function of trading activity allows us to achieve returns closer to IID normal  \n",
    ">Clark [1973] sampling returns by volume achieved even better statistical properties IID Gaussian. Forming data subordinated process of trading activity\n",
    "\n",
    "\n",
    "한 장 내외로 작성하려 줄였는데도, 그동안 크래프트의 영상을 보며 적용하고 싶었던 부분이 많아 글이 길어졌습니다. \n",
    "앞서 적었던 대로 금융시장에서 발생하는 여러 문제점들에 대해 영상에서 너무 잘 정리해주셔서 공부하는데 큰 도움이 되었습니다. \n",
    "베이지안 딥러닝을 사용하여 값 뿐만 아니라 값의 범위까지 같이 추정하는 크래프트의 모델은 정말 인상 깊었습니다 \n",
    "따라 구현해보고자 edwith에서 최성준님의 베이지안 딥러닝 강의와 VAE강의들도 듣고 확률론과 관련한 공부도 많이 했었습니다. <br> <br>\n",
    "딥러닝, 좀더 정확히는 텐서플로에 대한 공부를 시작한지 얼마 안되어서 코드가 조금 미숙한 부분이 있을 수 있습니다. 하지만 제가 좋아하는 것은 열정적으로 그리고 많이 공부할 수 있습니다. \n",
    "조금만 가르쳐 주시면 많은 시간을 투자해서 빠른 시일내에 능숙해지도록 하겠습니다. <br><br>\n",
    "감사합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'neptune'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e21a416ecea1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mneptune\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'neptune'"
     ]
    }
   ],
   "source": [
    "import neptune\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, Dot, Reshape, Dense, BatchNormalization, Dropout, LSTM, Activation, Flatten\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import os\n",
    "\n",
    "import matplotlib as mpl\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU  사용 가능\n"
     ]
    }
   ],
   "source": [
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.physical_device_desc for x in local_device_protos if x.device_type == 'GPU']\n",
    "print(\"GPU \", \"사용 가능\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"사용 불가능\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(asset_prices):\n",
    "    '''\n",
    "    필요한 값들 생성\n",
    "\n",
    "    :param asset_prices: (pd.DataFrame) Asset prices\n",
    "    '''\n",
    "    asset_name       = asset_prices.columns\n",
    "    number_of_assets = asset_name.size\n",
    "    time             = asset_prices.index\n",
    "    length_of_time   = time.size\n",
    "    first_weights    = np.ones(number_of_assets) / number_of_assets \n",
    "    all_weights      = np.zeros((length_of_time + 1, number_of_assets))\n",
    "\n",
    "    return asset_name, number_of_assets, time, length_of_time, first_weights, all_weights\n",
    "\n",
    "\n",
    "def calculate_return(asset_prices, resample_by=None):\n",
    "    \"\"\"\n",
    "    수익률 계산 , 기간 resample 가능하게 만들기\n",
    "\n",
    "    :param asset_prices: (pd.DataFrame) Asset prices\n",
    "    :param resample_by: (str) Period to resample data, None for no resampling\n",
    "    :return: (pd.DataFrame) Returns per asset\n",
    "    \"\"\"\n",
    "    if resample_by:\n",
    "        asset_prices = asset_prices.resample(resample_by).last()\n",
    "    asset_returns = asset_prices.pct_change().fillna(0)\n",
    "    return asset_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('price.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Datetime\n",
    "data.rename(columns={'Unnamed: 0': 'Date'}, inplace=True)\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "# Set Index\n",
    "data.set_index('Date', drop=True, inplace=True)\n",
    "asset_prices = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "asset_name = None\n",
    "number_of_assets = None\n",
    "time = None\n",
    "length_of_time = None\n",
    "first_weights = None\n",
    "all_weights = None\n",
    "\n",
    "asset_name, number_of_assets, time, length_of_time, first_weights, all_weights = initialize(asset_prices)\n",
    "monthly_return = calculate_return(asset_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = monthly_return[1:-12].copy()\n",
    "test_data = monthly_return[-24:].copy()\n",
    "test_data.drop(['B','N','P'],axis=1 ,inplace=True)\n",
    "\n",
    "training_data_array = np.array(training_data)\n",
    "test_data_array = np.array(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 데이터 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def purging_data(samples_info_sets: pd.Series,\n",
    "                 test_times: pd.Series) -> pd.Series:\n",
    "    '''\n",
    "    제가 기존에 사용하던 데이터는 학습 데이터와 라벨 사이의 길이가 전부 다르고 동시에 발생하기도 합니다,\n",
    "    때문에 데이터를 split할때 infomation leakage가 발생하지 않기 위해 매우 신경을 써야 했습니다.\n",
    "    Train starts within test & # Train ends within test & # Train envelops test를 drop해서 사용합니다\n",
    "    이후 Cross Validation을 할때도 사용되게 됩니다\n",
    "    CV시에는 추가적으로 테스트 셋 뒤쪽에 embargo도 넣어줍니다\n",
    "\n",
    "    '''\n",
    "    train = samples_info_sets.copy(deep=True)\n",
    "    for start_ix, end_ix in test_times.iteritems():\n",
    "        df0 = train[(start_ix <= train.index) & (train.index <= end_ix)].index  # Train starts within test\n",
    "        df1 = train[(start_ix <= train) & (train <= end_ix)].index  # Train ends within test\n",
    "        df2 = train[(train.index <= start_ix) & (end_ix <= train)].index  # Train envelops test\n",
    "        train = train.drop(df0.union(df1).union(df2))\n",
    "    return train\n",
    "\n",
    "\n",
    "class PurgedKFold():\n",
    "    '''\n",
    "    Purging 과 Embargo를 통해 K fold CV를 하면서도 information leakage를 막을 수 있습니다.\n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_splits: int = 3,\n",
    "                 samples_info_sets: pd.Series = None,\n",
    "                 pct_embargo: float = 0.):\n",
    "        self.n_splits = n_splits\n",
    "        self.samples_info_sets = samples_info_sets\n",
    "        self.pct_embargo = pct_embargo\n",
    "\n",
    "    def split(self,\n",
    "              X: pd.DataFrame,\n",
    "              y: pd.Series = None):\n",
    "\n",
    "        if X.shape[0] != self.samples_info_sets.shape[0]:\n",
    "            raise ValueError(\"X 와 'samples_info_sets' 의 길이가 다릅니다\")\n",
    "\n",
    "        indices: np.ndarray = np.arange(X.shape[0])\n",
    "        embargo: int = int(X.shape[0] * self.pct_embargo)\n",
    "\n",
    "        test_ranges: [(int, int)] = [(ix[0], ix[-1] + 1) for ix in np.array_split(np.arange(X.shape[0]), self.n_splits)]\n",
    "        for start_ix, end_ix in test_ranges:\n",
    "            test_indices = indices[start_ix:end_ix]\n",
    "            if end_ix < X.shape[0]:\n",
    "                end_ix += embargo\n",
    "            test_times = pd.Series(index=[self.samples_info_sets[start_ix]], data=[self.samples_info_sets[end_ix-1]])\n",
    "            train_times = purging_data(self.samples_info_sets, test_times)\n",
    "\n",
    "            train_indices = []\n",
    "            for train_ix in train_times.index:\n",
    "                train_indices.append(self.samples_info_sets.index.get_loc(train_ix))\n",
    "            yield np.array(train_indices), test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_points = 12\n",
    "sample_info_sets = pd.Series(index=training_data[:-history_points].index, data=training_data[history_points:].index)\n",
    "\n",
    "pct_embargo = 0.01\n",
    "cv_gen_purged = PurgedKFold(n_splits=4, samples_info_sets=sample_info_sets, pct_embargo=pct_embargo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_X_training_data = np.array([training_data_array[i:i+history_points].copy() for i in range(len(training_data_array) - history_points)])\n",
    "all_y_training_data = np.array([training_data_array[i + history_points].copy() for i in range(len(training_data_array) - history_points)])\n",
    "gen = cv_gen_purged.split(X=all_X_training_data, y=all_y_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid = next(gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 첫번째 gen : valid set(앞부분) train set(뒷부분) 둘 사이에 purge+embargo된 것 확인(111~126 index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
       "       140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152,\n",
       "       153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165,\n",
       "       166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178,\n",
       "       179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191,\n",
       "       192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204,\n",
       "       205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217,\n",
       "       218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230,\n",
       "       231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243,\n",
       "       244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256,\n",
       "       257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269,\n",
       "       270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282,\n",
       "       283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295,\n",
       "       296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308,\n",
       "       309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
       "       322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334,\n",
       "       335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347,\n",
       "       348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360,\n",
       "       361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373,\n",
       "       374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386,\n",
       "       387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399,\n",
       "       400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412,\n",
       "       413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425,\n",
       "       426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438,\n",
       "       439, 440, 441, 442, 443])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "       104, 105, 106, 107, 108, 109, 110])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid = next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "       104, 105, 106, 107, 108, 109, 110, 238, 239, 240, 241, 242, 243,\n",
       "       244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256,\n",
       "       257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269,\n",
       "       270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282,\n",
       "       283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295,\n",
       "       296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308,\n",
       "       309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
       "       322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334,\n",
       "       335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347,\n",
       "       348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360,\n",
       "       361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373,\n",
       "       374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386,\n",
       "       387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399,\n",
       "       400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412,\n",
       "       413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425,\n",
       "       426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438,\n",
       "       439, 440, 441, 442, 443])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123,\n",
       "       124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136,\n",
       "       137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149,\n",
       "       150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162,\n",
       "       163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175,\n",
       "       176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188,\n",
       "       189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201,\n",
       "       202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214,\n",
       "       215, 216, 217, 218, 219, 220, 221])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4번째 gen : 문제풀이를 위해 사용할 시간순서대로 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid = next(gen)\n",
    "train, valid = next(gen)\n",
    "train, valid = next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
       "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
       "       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
       "       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
       "       195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
       "       208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
       "       221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
       "       234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
       "       247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259,\n",
       "       260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272,\n",
       "       273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285,\n",
       "       286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298,\n",
       "       299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
       "       312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324,\n",
       "       325, 326, 327, 328, 329, 330, 331, 332])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345,\n",
       "       346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358,\n",
       "       359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371,\n",
       "       372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384,\n",
       "       385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397,\n",
       "       398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410,\n",
       "       411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423,\n",
       "       424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436,\n",
       "       437, 438, 439, 440, 441, 442, 443])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 학습 데이터 전처리하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input으로 수익률을 넣으라고 하셔서 따로 scaleing은 하지 않았습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 데이터를 train_set 과 valid_set로 나눠줌\n",
    "all_X_train_data = all_X_training_data[train]\n",
    "all_y_train_data = all_y_training_data[train]\n",
    "all_X_valid_data = all_X_training_data[valid]\n",
    "all_y_valid_data = all_y_training_data[valid]\n",
    "# Input 12개월, Label 1개월로 만들어줌\n",
    "X_train_data = np.array([all_X_train_data[j][:, i].copy() for i in range(number_of_assets) for j in range(len(train))])\n",
    "y_train_data = np.array([all_y_train_data[j][i].copy() for i in range(number_of_assets) for j in range(len(train))])\n",
    "X_valid_data = np.array([all_X_valid_data[j][:, i].copy() for i in range(number_of_assets) for j in range(len(valid))])\n",
    "y_valid_data = np.array([all_y_valid_data[j][i].copy() for i in range(number_of_assets) for j in range(len(valid))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NAN제거 : pct_change할경우 nan이 없어져서 price로 nan이 존재하는 동일한 위치 값 제거하였음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dele_nan_data = asset_prices[1:-12].copy()\n",
    "dele_nan_data_array = np.array(dele_nan_data)\n",
    "#\n",
    "del_X_training_data = np.array([dele_nan_data_array[i:i+history_points].copy() for i in range(len(dele_nan_data) - history_points)])\n",
    "del_y_training_data = np.array([dele_nan_data_array[i + history_points].copy() for i in range(len(dele_nan_data) - history_points)])\n",
    "#\n",
    "del_X_train_data = del_X_training_data[train]\n",
    "del_y_train_data = del_y_training_data[train]\n",
    "del_X_valid_data = del_X_training_data[valid]\n",
    "del_y_valid_data = del_y_training_data[valid]\n",
    "#\n",
    "del_nan_X_train_data = np.array([del_X_train_data[j][:, i].copy() for i in range(number_of_assets) for j in range(len(train))])\n",
    "del_nan_y_train_data = np.array([del_y_train_data[j][i].copy() for i in range(number_of_assets) for j in range(len(train))])\n",
    "del_nan_X_valid_data = np.array([del_X_valid_data[j][:, i].copy() for i in range(number_of_assets) for j in range(len(valid))])\n",
    "del_nan_y_valid_data = np.array([del_y_valid_data[j][i].copy() for i in range(number_of_assets) for j in range(len(valid))])\n",
    "# nan이 포함되지 않은 incices\n",
    "nan_indices_train = np.logical_not(np.logical_or(np.isnan(del_nan_y_train_data), np.isnan(del_nan_X_train_data).any(axis=1)))\n",
    "nan_indices_valid = np.logical_not(np.logical_or(np.isnan(del_nan_y_valid_data), np.isnan(del_nan_X_valid_data).any(axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nan제거 후 최종 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nan제거 후 최종 데이터\n",
    "X_train_data1 = X_train_data[nan_indices_train]\n",
    "y_train_data1 = y_train_data[nan_indices_train]\n",
    "X_valid_data1 = X_valid_data[nan_indices_valid]\n",
    "y_valid_data1 = y_valid_data[nan_indices_valid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 전처리 잘 됐나 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6993, 12) (6993,)\n"
     ]
    }
   ],
   "source": [
    "# Train : nan제거 전\n",
    "print(X_train_data.shape, y_train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6993, 12) (6993,)\n"
     ]
    }
   ],
   "source": [
    "# Train 데이터 nan제거 후 : nan 없음\n",
    "print(X_train_data1.shape, y_train_data1.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2331, 12) (2331,)\n"
     ]
    }
   ],
   "source": [
    "# Valid : nan제거 전\n",
    "print(X_valid_data.shape, y_valid_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2210, 12) (2210,)\n"
     ]
    }
   ],
   "source": [
    "# Valid데이터 nan제거 후 : nan 121개 존재\n",
    "print(X_valid_data1.shape, y_valid_data1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.Dataset 만들기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle?\n",
    "def data_input_train(x, y, BATCH_SIZE, BUFFER_SIZE):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    dataset = dataset.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "    return dataset\n",
    "\n",
    "def data_input_valid(x, y, BATCH_SIZE):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    dataset = dataset.batch(BATCH_SIZE).repeat()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_data1 = np.expand_dims(X_train_data1, -1)\n",
    "y_train_data1 = np.expand_dims(y_train_data1, -1)\n",
    "X_valid_data1 = np.expand_dims(X_valid_data1, -1)\n",
    "y_valid_data1 = np.expand_dims(y_valid_data1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "BUFFER_SIZE = 1000\n",
    "train_dataset = data_input_train(X_train_data1, y_train_data1, BATCH_SIZE, BUFFER_SIZE)\n",
    "valid_dataset = data_input_valid(X_valid_data1, y_valid_data1, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 모델과 콜백 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 프로젝트 관리는 넵튠으로 해왔습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeptuneLogger(Callback):\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        for log_name, log_value in logs.items():\n",
    "            neptune.log_metric(f'batch_{log_name}', log_value)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        for log_name, log_value in logs.items():\n",
    "            neptune.log_metric(f'epoch_{log_name}', log_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH_NR = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://ui.neptune.ai/sgeconomics/sandbox/e/SAN-40\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Experiment(SAN-40)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_NAME = 'test1'\n",
    "TOKEN = 'eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vdWkubmVwdHVuZS5haSIsImFwaV91cmwiOiJodHRwczovL3VpLm5lcHR1bmUuYWkiLCJhcGlfa2V5IjoiMzEyODExZTItMTJkOC00Mjk5LTgxNzItMjQwMGNjZGYwMGZmIn0='\n",
    "PARAMS ={\n",
    "        'Batch_size': BATCH_SIZE,\n",
    "        'Epoch_nr':   EPOCH_NR\n",
    "        }\n",
    "neptune.init('sgeconomics/sandbox',api_token = TOKEN)\n",
    "neptune.create_experiment(name=MODEL_NAME, params=PARAMS, tags=['adv'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- validation이 개선될때마다 weight를 저장하는 콜백입니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"training/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "cp_callback = ModelCheckpoint(filepath=checkpoint_path, monitor='val_loss', verbose=1, save_weights_only=True, save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모델은 최대한 간단하게 만들었습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictor(input_dataset):\n",
    "    print(tf.__version__)\n",
    "\n",
    "    lstm_input = Input(shape=input_dataset.shape[-2:], name='lstm_input')\n",
    "    x = LSTM(5, activation='relu', name='lstm_0')(lstm_input)\n",
    "    output = Dense(1, activation='sigmoid', name='dense_1')(x)\n",
    "\n",
    "    model = Model(inputs=lstm_input, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:1b:00.0, compute capability: 7.5',\n",
       " 'device: 1, name: GeForce RTX 2080 Ti, pci bus id: 0000:68:00.0, compute capability: 7.5']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "2.1.0\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "with strategy.scope():\n",
    "    model = predictor(X_train_data1)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.0005), loss='mse', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(model, 'model_with_shape_info.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 모델 학습하고 저장된 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 80 steps, validate for 40 steps\n",
      "Epoch 1/20\n",
      "INFO:tensorflow:batch_all_reduce: 5 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:batch_all_reduce: 5 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      " 1/80 [..............................] - ETA: 7:38 - loss: 0.2467 - accuracy: 0.0234WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.729248). Check your callbacks.\n",
      "78/80 [============================>.] - ETA: 0s - loss: 0.2366 - accuracy: 0.0070\n",
      "Epoch 00001: val_loss improved from inf to 0.21953, saving model to training/cp-0001.ckpt\n",
      "80/80 [==============================] - 10s 128ms/step - loss: 0.2361 - accuracy: 0.0071 - val_loss: 0.2195 - val_accuracy: 0.0124\n",
      "Epoch 2/20\n",
      "79/80 [============================>.] - ETA: 0s - loss: 0.1822 - accuracy: 0.0070\n",
      "Epoch 00002: val_loss improved from 0.21953 to 0.11581, saving model to training/cp-0002.ckpt\n",
      "80/80 [==============================] - 3s 33ms/step - loss: 0.1814 - accuracy: 0.0069 - val_loss: 0.1158 - val_accuracy: 0.0124\n",
      "Epoch 3/20\n",
      "79/80 [============================>.] - ETA: 0s - loss: 0.0257 - accuracy: 0.0077\n",
      "Epoch 00003: val_loss improved from 0.11581 to 0.00166, saving model to training/cp-0003.ckpt\n",
      "80/80 [==============================] - 3s 34ms/step - loss: 0.0254 - accuracy: 0.0076 - val_loss: 0.0017 - val_accuracy: 0.0124\n",
      "Epoch 4/20\n",
      "78/80 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.0060\n",
      "Epoch 00004: val_loss improved from 0.00166 to 0.00163, saving model to training/cp-0004.ckpt\n",
      "80/80 [==============================] - 3s 35ms/step - loss: 0.0016 - accuracy: 0.0063 - val_loss: 0.0016 - val_accuracy: 0.0124\n",
      "Epoch 5/20\n",
      "78/80 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.0076\n",
      "Epoch 00005: val_loss improved from 0.00163 to 0.00162, saving model to training/cp-0005.ckpt\n",
      "80/80 [==============================] - 3s 33ms/step - loss: 0.0015 - accuracy: 0.0074 - val_loss: 0.0016 - val_accuracy: 0.0124\n",
      "Epoch 6/20\n",
      "79/80 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.0068\n",
      "Epoch 00006: val_loss improved from 0.00162 to 0.00162, saving model to training/cp-0006.ckpt\n",
      "80/80 [==============================] - 3s 33ms/step - loss: 0.0015 - accuracy: 0.0068 - val_loss: 0.0016 - val_accuracy: 0.0124\n",
      "Epoch 7/20\n",
      "78/80 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.0079\n",
      "Epoch 00007: val_loss improved from 0.00162 to 0.00161, saving model to training/cp-0007.ckpt\n",
      "80/80 [==============================] - 3s 34ms/step - loss: 0.0015 - accuracy: 0.0077 - val_loss: 0.0016 - val_accuracy: 0.0124\n",
      "Epoch 8/20\n",
      "79/80 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 0.0065\n",
      "Epoch 00008: val_loss improved from 0.00161 to 0.00161, saving model to training/cp-0008.ckpt\n",
      "80/80 [==============================] - 3s 34ms/step - loss: 0.0015 - accuracy: 0.0065 - val_loss: 0.0016 - val_accuracy: 0.0124\n",
      "Epoch 9/20\n",
      "78/80 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.0073\n",
      "Epoch 00009: val_loss improved from 0.00161 to 0.00161, saving model to training/cp-0009.ckpt\n",
      "80/80 [==============================] - 3s 33ms/step - loss: 0.0015 - accuracy: 0.0073 - val_loss: 0.0016 - val_accuracy: 0.0124\n",
      "Epoch 10/20\n",
      "78/80 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.0066\n",
      "Epoch 00010: val_loss improved from 0.00161 to 0.00161, saving model to training/cp-0010.ckpt\n",
      "80/80 [==============================] - 3s 33ms/step - loss: 0.0015 - accuracy: 0.0066 - val_loss: 0.0016 - val_accuracy: 0.0124\n",
      "Epoch 11/20\n",
      "79/80 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.0070\n",
      "Epoch 00011: val_loss improved from 0.00161 to 0.00161, saving model to training/cp-0011.ckpt\n",
      "80/80 [==============================] - 3s 34ms/step - loss: 0.0015 - accuracy: 0.0069 - val_loss: 0.0016 - val_accuracy: 0.0124\n",
      "Epoch 12/20\n",
      "78/80 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.0072\n",
      "Epoch 00012: val_loss improved from 0.00161 to 0.00161, saving model to training/cp-0012.ckpt\n",
      "80/80 [==============================] - 3s 34ms/step - loss: 0.0015 - accuracy: 0.0073 - val_loss: 0.0016 - val_accuracy: 0.0124\n",
      "Epoch 13/20\n",
      "78/80 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.0069\n",
      "Epoch 00013: val_loss improved from 0.00161 to 0.00161, saving model to training/cp-0013.ckpt\n",
      "80/80 [==============================] - 3s 34ms/step - loss: 0.0015 - accuracy: 0.0069 - val_loss: 0.0016 - val_accuracy: 0.0124\n",
      "Epoch 14/20\n",
      "79/80 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.0074\n",
      "Epoch 00014: val_loss improved from 0.00161 to 0.00161, saving model to training/cp-0014.ckpt\n",
      "80/80 [==============================] - 3s 33ms/step - loss: 0.0015 - accuracy: 0.0074 - val_loss: 0.0016 - val_accuracy: 0.0124\n",
      "Epoch 15/20\n",
      "79/80 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.0067\n",
      "Epoch 00015: val_loss improved from 0.00161 to 0.00161, saving model to training/cp-0015.ckpt\n",
      "80/80 [==============================] - 3s 33ms/step - loss: 0.0015 - accuracy: 0.0066 - val_loss: 0.0016 - val_accuracy: 0.0124\n",
      "Epoch 16/20\n",
      "78/80 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.0073\n",
      "Epoch 00016: val_loss improved from 0.00161 to 0.00161, saving model to training/cp-0016.ckpt\n",
      "80/80 [==============================] - 3s 33ms/step - loss: 0.0015 - accuracy: 0.0073 - val_loss: 0.0016 - val_accuracy: 0.0124\n",
      "Epoch 17/20\n",
      "78/80 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.0072\n",
      "Epoch 00017: val_loss improved from 0.00161 to 0.00161, saving model to training/cp-0017.ckpt\n",
      "80/80 [==============================] - 3s 33ms/step - loss: 0.0015 - accuracy: 0.0071 - val_loss: 0.0016 - val_accuracy: 0.0124\n",
      "Epoch 18/20\n",
      "79/80 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.0075\n",
      "Epoch 00018: val_loss did not improve from 0.00161\n",
      "80/80 [==============================] - 3s 34ms/step - loss: 0.0015 - accuracy: 0.0074 - val_loss: 0.0016 - val_accuracy: 0.0124\n",
      "Epoch 19/20\n",
      "79/80 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.0068 ETA: 0s - loss: 0.0015 - accu\n",
      "Epoch 00019: val_loss did not improve from 0.00161\n",
      "80/80 [==============================] - 3s 34ms/step - loss: 0.0015 - accuracy: 0.0068 - val_loss: 0.0016 - val_accuracy: 0.0124\n",
      "Epoch 20/20\n",
      "79/80 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.0068\n",
      "Epoch 00020: val_loss improved from 0.00161 to 0.00161, saving model to training/cp-0020.ckpt\n",
      "80/80 [==============================] - 3s 34ms/step - loss: 0.0015 - accuracy: 0.0071 - val_loss: 0.0016 - val_accuracy: 0.0124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fda5b7cb668>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dataset,\n",
    "          epochs=EPOCH_NR,\n",
    "          steps_per_epoch=80,\n",
    "          validation_data=valid_dataset,\n",
    "          validation_steps=40,\n",
    "          verbose=1,\n",
    "          callbacks=[NeptuneLogger(), cp_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 가장 최근에 저장된 모델을 불러옵니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'training/cp-0020.ckpt'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fda302b9c50>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = predictor(X_train_data1)\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "model.load_weights(latest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 학습된 모델로 마지막 12달 가격 예측하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_data = np.array([test_data_array[i:i+history_points].copy() for i in range(len(test_data) - history_points)])\n",
    "X_test = np.array([X_test_data[j][:, i].copy() for i in range(X_test_data.shape[-1]) for j in range(X_test_data.shape[-2])])\n",
    "X_test1 = np.expand_dims(X_test, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = pd.DataFrame(model.predict(X_test1).reshape(X_test_data.shape[-1],X_test_data.shape[-2]).T , columns=test_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>F</th>\n",
       "      <th>G</th>\n",
       "      <th>H</th>\n",
       "      <th>I</th>\n",
       "      <th>J</th>\n",
       "      <th>K</th>\n",
       "      <th>L</th>\n",
       "      <th>M</th>\n",
       "      <th>O</th>\n",
       "      <th>Q</th>\n",
       "      <th>R</th>\n",
       "      <th>S</th>\n",
       "      <th>T</th>\n",
       "      <th>U</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001860</td>\n",
       "      <td>0.001922</td>\n",
       "      <td>0.001837</td>\n",
       "      <td>0.002352</td>\n",
       "      <td>0.002055</td>\n",
       "      <td>0.002808</td>\n",
       "      <td>0.002343</td>\n",
       "      <td>0.001847</td>\n",
       "      <td>0.002147</td>\n",
       "      <td>0.002332</td>\n",
       "      <td>0.001683</td>\n",
       "      <td>0.002910</td>\n",
       "      <td>0.001730</td>\n",
       "      <td>0.001924</td>\n",
       "      <td>0.003629</td>\n",
       "      <td>0.002230</td>\n",
       "      <td>0.001864</td>\n",
       "      <td>0.001805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001641</td>\n",
       "      <td>0.001812</td>\n",
       "      <td>0.001958</td>\n",
       "      <td>0.002420</td>\n",
       "      <td>0.002098</td>\n",
       "      <td>0.002306</td>\n",
       "      <td>0.002460</td>\n",
       "      <td>0.001674</td>\n",
       "      <td>0.002417</td>\n",
       "      <td>0.003268</td>\n",
       "      <td>0.002246</td>\n",
       "      <td>0.002896</td>\n",
       "      <td>0.001677</td>\n",
       "      <td>0.002393</td>\n",
       "      <td>0.002997</td>\n",
       "      <td>0.002307</td>\n",
       "      <td>0.001818</td>\n",
       "      <td>0.001588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001411</td>\n",
       "      <td>0.001648</td>\n",
       "      <td>0.002078</td>\n",
       "      <td>0.002039</td>\n",
       "      <td>0.001715</td>\n",
       "      <td>0.002313</td>\n",
       "      <td>0.002127</td>\n",
       "      <td>0.001504</td>\n",
       "      <td>0.001988</td>\n",
       "      <td>0.003635</td>\n",
       "      <td>0.002866</td>\n",
       "      <td>0.002450</td>\n",
       "      <td>0.001595</td>\n",
       "      <td>0.002182</td>\n",
       "      <td>0.002926</td>\n",
       "      <td>0.002915</td>\n",
       "      <td>0.001599</td>\n",
       "      <td>0.001710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001098</td>\n",
       "      <td>0.001748</td>\n",
       "      <td>0.002308</td>\n",
       "      <td>0.002184</td>\n",
       "      <td>0.001649</td>\n",
       "      <td>0.001792</td>\n",
       "      <td>0.001979</td>\n",
       "      <td>0.001291</td>\n",
       "      <td>0.001825</td>\n",
       "      <td>0.003280</td>\n",
       "      <td>0.002646</td>\n",
       "      <td>0.002208</td>\n",
       "      <td>0.001159</td>\n",
       "      <td>0.001626</td>\n",
       "      <td>0.002648</td>\n",
       "      <td>0.002588</td>\n",
       "      <td>0.001215</td>\n",
       "      <td>0.001840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001055</td>\n",
       "      <td>0.001583</td>\n",
       "      <td>0.002049</td>\n",
       "      <td>0.002032</td>\n",
       "      <td>0.001662</td>\n",
       "      <td>0.001959</td>\n",
       "      <td>0.002209</td>\n",
       "      <td>0.001440</td>\n",
       "      <td>0.001852</td>\n",
       "      <td>0.003784</td>\n",
       "      <td>0.002535</td>\n",
       "      <td>0.001970</td>\n",
       "      <td>0.001769</td>\n",
       "      <td>0.001397</td>\n",
       "      <td>0.003240</td>\n",
       "      <td>0.002459</td>\n",
       "      <td>0.001279</td>\n",
       "      <td>0.002160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.001108</td>\n",
       "      <td>0.001916</td>\n",
       "      <td>0.002096</td>\n",
       "      <td>0.001830</td>\n",
       "      <td>0.001594</td>\n",
       "      <td>0.002359</td>\n",
       "      <td>0.002064</td>\n",
       "      <td>0.001812</td>\n",
       "      <td>0.002166</td>\n",
       "      <td>0.003715</td>\n",
       "      <td>0.002335</td>\n",
       "      <td>0.002743</td>\n",
       "      <td>0.002026</td>\n",
       "      <td>0.001653</td>\n",
       "      <td>0.002847</td>\n",
       "      <td>0.002175</td>\n",
       "      <td>0.001504</td>\n",
       "      <td>0.002426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.001025</td>\n",
       "      <td>0.001792</td>\n",
       "      <td>0.002217</td>\n",
       "      <td>0.001499</td>\n",
       "      <td>0.001914</td>\n",
       "      <td>0.002179</td>\n",
       "      <td>0.001596</td>\n",
       "      <td>0.001346</td>\n",
       "      <td>0.002261</td>\n",
       "      <td>0.003048</td>\n",
       "      <td>0.002019</td>\n",
       "      <td>0.001992</td>\n",
       "      <td>0.001948</td>\n",
       "      <td>0.001094</td>\n",
       "      <td>0.001907</td>\n",
       "      <td>0.002074</td>\n",
       "      <td>0.001552</td>\n",
       "      <td>0.001998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.001134</td>\n",
       "      <td>0.001509</td>\n",
       "      <td>0.001836</td>\n",
       "      <td>0.001468</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.002389</td>\n",
       "      <td>0.002058</td>\n",
       "      <td>0.001407</td>\n",
       "      <td>0.001956</td>\n",
       "      <td>0.002998</td>\n",
       "      <td>0.001689</td>\n",
       "      <td>0.002026</td>\n",
       "      <td>0.002114</td>\n",
       "      <td>0.001568</td>\n",
       "      <td>0.001353</td>\n",
       "      <td>0.002355</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.002550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.001275</td>\n",
       "      <td>0.002196</td>\n",
       "      <td>0.002534</td>\n",
       "      <td>0.001413</td>\n",
       "      <td>0.001786</td>\n",
       "      <td>0.002673</td>\n",
       "      <td>0.001965</td>\n",
       "      <td>0.002147</td>\n",
       "      <td>0.001843</td>\n",
       "      <td>0.003197</td>\n",
       "      <td>0.001635</td>\n",
       "      <td>0.001762</td>\n",
       "      <td>0.002572</td>\n",
       "      <td>0.001896</td>\n",
       "      <td>0.001541</td>\n",
       "      <td>0.002541</td>\n",
       "      <td>0.001638</td>\n",
       "      <td>0.002593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.001238</td>\n",
       "      <td>0.001427</td>\n",
       "      <td>0.002398</td>\n",
       "      <td>0.001261</td>\n",
       "      <td>0.001536</td>\n",
       "      <td>0.002333</td>\n",
       "      <td>0.001886</td>\n",
       "      <td>0.001966</td>\n",
       "      <td>0.001601</td>\n",
       "      <td>0.003176</td>\n",
       "      <td>0.001630</td>\n",
       "      <td>0.001381</td>\n",
       "      <td>0.002052</td>\n",
       "      <td>0.001653</td>\n",
       "      <td>0.001302</td>\n",
       "      <td>0.002182</td>\n",
       "      <td>0.002065</td>\n",
       "      <td>0.002156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.001169</td>\n",
       "      <td>0.001863</td>\n",
       "      <td>0.002635</td>\n",
       "      <td>0.001446</td>\n",
       "      <td>0.001877</td>\n",
       "      <td>0.002498</td>\n",
       "      <td>0.001826</td>\n",
       "      <td>0.002534</td>\n",
       "      <td>0.002282</td>\n",
       "      <td>0.003465</td>\n",
       "      <td>0.001839</td>\n",
       "      <td>0.001653</td>\n",
       "      <td>0.001804</td>\n",
       "      <td>0.001499</td>\n",
       "      <td>0.001640</td>\n",
       "      <td>0.002313</td>\n",
       "      <td>0.002576</td>\n",
       "      <td>0.002018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.001261</td>\n",
       "      <td>0.002154</td>\n",
       "      <td>0.002392</td>\n",
       "      <td>0.001781</td>\n",
       "      <td>0.001596</td>\n",
       "      <td>0.002984</td>\n",
       "      <td>0.001447</td>\n",
       "      <td>0.002483</td>\n",
       "      <td>0.002275</td>\n",
       "      <td>0.003591</td>\n",
       "      <td>0.002637</td>\n",
       "      <td>0.001778</td>\n",
       "      <td>0.001952</td>\n",
       "      <td>0.001630</td>\n",
       "      <td>0.001510</td>\n",
       "      <td>0.002049</td>\n",
       "      <td>0.002399</td>\n",
       "      <td>0.002136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           A         C         D         E         F         G         H  \\\n",
       "0   0.001860  0.001922  0.001837  0.002352  0.002055  0.002808  0.002343   \n",
       "1   0.001641  0.001812  0.001958  0.002420  0.002098  0.002306  0.002460   \n",
       "2   0.001411  0.001648  0.002078  0.002039  0.001715  0.002313  0.002127   \n",
       "3   0.001098  0.001748  0.002308  0.002184  0.001649  0.001792  0.001979   \n",
       "4   0.001055  0.001583  0.002049  0.002032  0.001662  0.001959  0.002209   \n",
       "5   0.001108  0.001916  0.002096  0.001830  0.001594  0.002359  0.002064   \n",
       "6   0.001025  0.001792  0.002217  0.001499  0.001914  0.002179  0.001596   \n",
       "7   0.001134  0.001509  0.001836  0.001468  0.001563  0.002389  0.002058   \n",
       "8   0.001275  0.002196  0.002534  0.001413  0.001786  0.002673  0.001965   \n",
       "9   0.001238  0.001427  0.002398  0.001261  0.001536  0.002333  0.001886   \n",
       "10  0.001169  0.001863  0.002635  0.001446  0.001877  0.002498  0.001826   \n",
       "11  0.001261  0.002154  0.002392  0.001781  0.001596  0.002984  0.001447   \n",
       "\n",
       "           I         J         K         L         M         O         Q  \\\n",
       "0   0.001847  0.002147  0.002332  0.001683  0.002910  0.001730  0.001924   \n",
       "1   0.001674  0.002417  0.003268  0.002246  0.002896  0.001677  0.002393   \n",
       "2   0.001504  0.001988  0.003635  0.002866  0.002450  0.001595  0.002182   \n",
       "3   0.001291  0.001825  0.003280  0.002646  0.002208  0.001159  0.001626   \n",
       "4   0.001440  0.001852  0.003784  0.002535  0.001970  0.001769  0.001397   \n",
       "5   0.001812  0.002166  0.003715  0.002335  0.002743  0.002026  0.001653   \n",
       "6   0.001346  0.002261  0.003048  0.002019  0.001992  0.001948  0.001094   \n",
       "7   0.001407  0.001956  0.002998  0.001689  0.002026  0.002114  0.001568   \n",
       "8   0.002147  0.001843  0.003197  0.001635  0.001762  0.002572  0.001896   \n",
       "9   0.001966  0.001601  0.003176  0.001630  0.001381  0.002052  0.001653   \n",
       "10  0.002534  0.002282  0.003465  0.001839  0.001653  0.001804  0.001499   \n",
       "11  0.002483  0.002275  0.003591  0.002637  0.001778  0.001952  0.001630   \n",
       "\n",
       "           R         S         T         U  \n",
       "0   0.003629  0.002230  0.001864  0.001805  \n",
       "1   0.002997  0.002307  0.001818  0.001588  \n",
       "2   0.002926  0.002915  0.001599  0.001710  \n",
       "3   0.002648  0.002588  0.001215  0.001840  \n",
       "4   0.003240  0.002459  0.001279  0.002160  \n",
       "5   0.002847  0.002175  0.001504  0.002426  \n",
       "6   0.001907  0.002074  0.001552  0.001998  \n",
       "7   0.001353  0.002355  0.001386  0.002550  \n",
       "8   0.001541  0.002541  0.001638  0.002593  \n",
       "9   0.001302  0.002182  0.002065  0.002156  \n",
       "10  0.001640  0.002313  0.002576  0.002018  \n",
       "11  0.001510  0.002049  0.002399  0.002136  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted.to_csv('predict.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
