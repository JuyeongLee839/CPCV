{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import datetime \n",
    "\n",
    "from CV import cross_validation as CV\n",
    "from CV import combinatorial as CB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Order \n",
    "1. X, Y generation \n",
    "2. Train(valid) / Test splitting\n",
    "3. CPCV\n",
    "   어차피 여기서 뒤에서 개수만큼 잘라주는 거면 데이터 포인트를 자르는 것과 다를바 없음\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/data_input_demo.csv\", index_col = [0])\n",
    "df = df.set_index(['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>13ty_index</th>\n",
       "      <th>interty_index</th>\n",
       "      <th>lty_index</th>\n",
       "      <th>mbs_index</th>\n",
       "      <th>13cy_index</th>\n",
       "      <th>intercy_index</th>\n",
       "      <th>lcy_index</th>\n",
       "      <th>ty_index</th>\n",
       "      <th>cy_index</th>\n",
       "      <th>agg_index</th>\n",
       "      <th>real_known</th>\n",
       "      <th>cat_obs1</th>\n",
       "      <th>cat_obs2</th>\n",
       "      <th>cat_knwon1</th>\n",
       "      <th>cat_knwon2</th>\n",
       "      <th>static</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1997-05-19</th>\n",
       "      <td>133.46</td>\n",
       "      <td>784.22</td>\n",
       "      <td>840.64</td>\n",
       "      <td>751.37</td>\n",
       "      <td>668.10</td>\n",
       "      <td>893.58</td>\n",
       "      <td>912.47</td>\n",
       "      <td>824.23</td>\n",
       "      <td>861.00</td>\n",
       "      <td>715.66</td>\n",
       "      <td>215.632652</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-05-20</th>\n",
       "      <td>133.58</td>\n",
       "      <td>785.01</td>\n",
       "      <td>840.34</td>\n",
       "      <td>751.82</td>\n",
       "      <td>668.74</td>\n",
       "      <td>894.56</td>\n",
       "      <td>912.29</td>\n",
       "      <td>824.78</td>\n",
       "      <td>861.51</td>\n",
       "      <td>716.09</td>\n",
       "      <td>980.112727</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-05-21</th>\n",
       "      <td>133.59</td>\n",
       "      <td>784.63</td>\n",
       "      <td>836.47</td>\n",
       "      <td>751.97</td>\n",
       "      <td>668.77</td>\n",
       "      <td>893.58</td>\n",
       "      <td>908.76</td>\n",
       "      <td>823.54</td>\n",
       "      <td>859.71</td>\n",
       "      <td>715.31</td>\n",
       "      <td>247.766286</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-05-22</th>\n",
       "      <td>133.58</td>\n",
       "      <td>784.33</td>\n",
       "      <td>834.66</td>\n",
       "      <td>751.97</td>\n",
       "      <td>668.71</td>\n",
       "      <td>892.96</td>\n",
       "      <td>906.86</td>\n",
       "      <td>822.86</td>\n",
       "      <td>858.69</td>\n",
       "      <td>714.88</td>\n",
       "      <td>306.038065</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-05-23</th>\n",
       "      <td>133.58</td>\n",
       "      <td>784.70</td>\n",
       "      <td>835.59</td>\n",
       "      <td>752.27</td>\n",
       "      <td>668.71</td>\n",
       "      <td>893.58</td>\n",
       "      <td>907.77</td>\n",
       "      <td>823.37</td>\n",
       "      <td>859.37</td>\n",
       "      <td>715.31</td>\n",
       "      <td>944.644883</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            13ty_index  interty_index  lty_index  mbs_index  13cy_index  \\\n",
       "date                                                                      \n",
       "1997-05-19      133.46         784.22     840.64     751.37      668.10   \n",
       "1997-05-20      133.58         785.01     840.34     751.82      668.74   \n",
       "1997-05-21      133.59         784.63     836.47     751.97      668.77   \n",
       "1997-05-22      133.58         784.33     834.66     751.97      668.71   \n",
       "1997-05-23      133.58         784.70     835.59     752.27      668.71   \n",
       "\n",
       "            intercy_index  lcy_index  ty_index  cy_index  agg_index  \\\n",
       "date                                                                  \n",
       "1997-05-19         893.58     912.47    824.23    861.00     715.66   \n",
       "1997-05-20         894.56     912.29    824.78    861.51     716.09   \n",
       "1997-05-21         893.58     908.76    823.54    859.71     715.31   \n",
       "1997-05-22         892.96     906.86    822.86    858.69     714.88   \n",
       "1997-05-23         893.58     907.77    823.37    859.37     715.31   \n",
       "\n",
       "            real_known  cat_obs1  cat_obs2  cat_knwon1  cat_knwon2  static  \n",
       "date                                                                        \n",
       "1997-05-19  215.632652         2         2           4           3     1.0  \n",
       "1997-05-20  980.112727         2         1           5           4     1.0  \n",
       "1997-05-21  247.766286         2         1           4           4     1.0  \n",
       "1997-05-22  306.038065         2         1           3           4     1.0  \n",
       "1997-05-23  944.644883         0         2           3           3     1.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X, Y generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_xy_seq(df: pd.DataFrame, x_seq = 66, y_seq = 22):\n",
    "    \"\"\"\n",
    "    Generate samples from\n",
    "    :param df:\n",
    "    :param x_seq:\n",
    "    :param y_seq:\n",
    "    :param scaler:\n",
    "    :return:\n",
    "    # x: (epoch_size, input_length, num_nodes, input_dim)\n",
    "    # y: (epoch_size, output_length, num_nodes, output_dim)\n",
    "    \"\"\"\n",
    "    num_samples, num_nodes = df.shape\n",
    "    dates_arr = np.array(df.index)\n",
    "    data = np.expand_dims(df.values, axis = -1) # df -> array [N, F, 1]\n",
    "\n",
    "    x_offsets = np.arange(-x_seq+1, 1)  \n",
    "    y_offsets = np.arange(1, y_seq+1)\n",
    "\n",
    "    # feature_list = [data]\n",
    "\n",
    "    x, y = [], []\n",
    "    x_date, y_date = [],[]\n",
    "\n",
    "    min_t = abs(min(x_offsets))\n",
    "    max_t = abs(num_samples - abs(max(y_offsets)))\n",
    "\n",
    "    for t in range(min_t, max_t):\n",
    "        # value seperation\n",
    "        x.append(data[t+x_offsets, ...])\n",
    "        y.append(data[t+y_offsets, ...])\n",
    "        # date seperation\n",
    "        x_date.append(dates_arr[t+x_offsets])\n",
    "        y_date.append(dates_arr[t+y_offsets])\n",
    "        \n",
    "    x = np.stack(x, axis = 0)\n",
    "    y = np.stack(y, axis = 0)\n",
    "\n",
    "    x_date = np.stack(x_date, axis = 0)\n",
    "    y_date = np.stack(y_date, axis = 0)\n",
    "\n",
    "    return x, y, x_date, y_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, x_date, y_date = generate_xy_seq(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6298, 66, 16, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6298, 66)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_date.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = df.shape[0]\n",
    "num_train = round(num_samples * 0.8)\n",
    "# num_val = round(num_train * 0.2)\n",
    "# num_train = num_train - num_val   \n",
    "num_test = num_samples - (num_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = df.iloc[:num_train]\n",
    "TEST = df.iloc[num_train : ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPCV\n",
    "- 6C2\n",
    "- purging\n",
    "- embargo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(asset_prices):\n",
    "    '''\n",
    "    필요한 값들 생성\n",
    "\n",
    "    :param asset_prices: (pd.DataFrame) Asset prices\n",
    "    '''\n",
    "    asset_name       = asset_prices.columns\n",
    "    number_of_assets = asset_name.size\n",
    "    time             = asset_prices.index\n",
    "    length_of_time   = time.size\n",
    "    first_weights    = np.ones(number_of_assets) / number_of_assets \n",
    "    all_weights      = np.zeros((length_of_time + 1, number_of_assets))\n",
    "\n",
    "    return asset_name, number_of_assets, time, length_of_time, first_weights, all_weights\n",
    "\n",
    "\n",
    "def calculate_return(asset_prices, resample_by=None):\n",
    "    \"\"\"\n",
    "    수익률 계산 , 기간 resample 가능하게 만들기\n",
    "\n",
    "    :param asset_prices: (pd.DataFrame) Asset prices\n",
    "    :param resample_by: (str) Period to resample data, None for no resampling\n",
    "    :return: (pd.DataFrame) Returns per asset\n",
    "    \"\"\"\n",
    "    if resample_by:\n",
    "        asset_prices = asset_prices.resample(resample_by).last()\n",
    "    asset_returns = asset_prices.pct_change().fillna(0)\n",
    "    return asset_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "asset_prices = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "asset_name = None\n",
    "number_of_assets = None\n",
    "time = None\n",
    "length_of_time = None\n",
    "first_weights = None\n",
    "all_weights = None\n",
    "\n",
    "asset_name, number_of_assets, time, length_of_time, first_weights, all_weights = initialize(asset_prices)\n",
    "monthly_return = calculate_return(asset_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = monthly_return[1:-12].copy() # 마지막 12 제외\n",
    "test_data = monthly_return[-24:].copy() # 마지막 24 부터 시작\n",
    "# test_data.drop(['B','N','P'],axis=1 ,inplace=True)\n",
    "training_data_array = np.array(training_data)\n",
    "test_data_array = np.array(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터분리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.model_selection import BaseCrossValidator\n",
    "\n",
    "\n",
    "def ml_get_train_times(samples_info_sets: pd.Series, test_times: pd.Series) -> pd.Series:\n",
    "    # pylint: disable=invalid-name\n",
    "    \"\"\"\n",
    "    Advances in Financial Machine Learning, Snippet 7.1, page 106.\n",
    "\n",
    "    Purging observations in the training set\n",
    "\n",
    "    This function find the training set indexes given the information on which each record is based\n",
    "    and the range for the test set.\n",
    "    Given test_times, find the times of the training observations.\n",
    "\n",
    "    :param samples_info_sets: (pd.Series) The information range on which each record is constructed from\n",
    "        *samples_info_sets.index*: Time when the information extraction started.\n",
    "        *samples_info_sets.value*: Time when the information extraction ended.\n",
    "    :param test_times: (pd.Series) Times for the test dataset.\n",
    "    :return: (pd.Series) Training set\n",
    "    \"\"\"\n",
    "    train = samples_info_sets.copy(deep=True)\n",
    "    # train.index : train start index \n",
    "    # train : train end index\n",
    "    \n",
    "    for start_ix, end_ix in test_times.iteritems():\n",
    "        df0 = train[(start_ix <= train.index) & (train.index <= end_ix)].index  # Train starts within test\n",
    "        df1 = train[(start_ix <= train) & (train <= end_ix)].index  # Train ends within test\n",
    "        df2 = train[(train.index <= start_ix) & (end_ix <= train)].index  # Train envelops test\n",
    "        train = train.drop(df0.union(df1).union(df2))\n",
    "        \n",
    "    return train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implements the Combinatorial Purged Cross-Validation class from Chapter 12\n",
    "\"\"\"\n",
    "import sys \n",
    "from itertools import combinations\n",
    "from typing import List\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy.special import comb\n",
    "from sklearn.model_selection import KFold\n",
    "# from .cross_validation import ml_get_train_times\n",
    "\n",
    "\n",
    "def _get_number_of_backtest_paths(n_train_splits: int, n_test_splits: int) -> float:\n",
    "    \"\"\"\n",
    "    Number of combinatorial paths for CPCV(N,K)\n",
    "    :param n_train_splits: (int) number of train splits\n",
    "    :param n_test_splits: (int) number of test splits\n",
    "    :return: (int) number of backtest paths for CPCV(N,k)\n",
    "    \"\"\"\n",
    "    return int(comb(n_train_splits, n_train_splits - n_test_splits) * n_test_splits / n_train_splits)\n",
    "\n",
    "\n",
    "class CombinatorialPurgedKFold(KFold):\n",
    "    \"\"\"\n",
    "    Advances in Financial Machine Learning, Chapter 12.\n",
    "\n",
    "    Implements Combinatial Purged Cross Validation (CPCV)\n",
    "\n",
    "    The train is purged of observations overlapping test-label intervals\n",
    "    Test set is assumed contiguous (shuffle=False), w/o training samples in between\n",
    "\n",
    "    :param n_splits: (int) The number of splits. Default to 3\n",
    "    :param samples_info_sets: (pd.Series) The information range on which each record is constructed from\n",
    "        *samples_info_sets.index*: Time when the information extraction started.\n",
    "        *samples_info_sets.value*: Time when the information extraction ended.\n",
    "    :param pct_embargo: (float) Percent that determines the embargo size.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_splits: int = 3,\n",
    "                 n_test_splits: int = 2,\n",
    "                 samples_info_sets: pd.Series = None,\n",
    "                 pct_embargo: float = 0.):\n",
    "\n",
    "        if not isinstance(samples_info_sets, pd.Series):\n",
    "            raise ValueError('The samples_info_sets param must be a pd.Series')\n",
    "        super(CombinatorialPurgedKFold, self).__init__(n_splits, shuffle=False, random_state=None)\n",
    "\n",
    "        self.samples_info_sets = samples_info_sets\n",
    "        self.pct_embargo = pct_embargo\n",
    "        self.n_test_splits = n_test_splits\n",
    "        self.num_backtest_paths = _get_number_of_backtest_paths(self.n_splits, self.n_test_splits)\n",
    "        self.backtest_paths = []  # Array of backtest paths\n",
    "\n",
    "    def _generate_combinatorial_test_ranges(self, splits_indices: dict) -> List:\n",
    "        \"\"\"\n",
    "        Using start and end indices of test splits from KFolds and number of test_splits (self.n_test_splits),\n",
    "        generates combinatorial test ranges splits\n",
    "\n",
    "        :param splits_indices: (dict) Test fold integer index: [start test index, end test index]\n",
    "        :return: (list) Combinatorial test splits ([start index, end index])\n",
    "        \"\"\"\n",
    "\n",
    "        # Possible test splits for each fold\n",
    "        combinatorial_splits = list(combinations(list(splits_indices.keys()), self.n_test_splits))\n",
    "        combinatorial_test_ranges = []  # List of test indices formed from combinatorial splits\n",
    "        for combination in combinatorial_splits:\n",
    "            temp_test_indices = []  # Array of test indices for current split combination\n",
    "            for int_index in combination:\n",
    "                temp_test_indices.append(splits_indices[int_index])\n",
    "            combinatorial_test_ranges.append(temp_test_indices)\n",
    "        return combinatorial_test_ranges\n",
    "\n",
    "    def _fill_backtest_paths(self, train_indices: list, test_splits: list):\n",
    "        \"\"\"\n",
    "        Using start and end indices of test splits and purged/embargoed train indices from CPCV, find backtest path and\n",
    "        place in the path where these indices should be used.\n",
    "\n",
    "        :param test_splits: (list) of lists with first element corresponding to test start index and second - test end\n",
    "        \"\"\"\n",
    "        # Fill backtest paths using train/test splits from CPCV\n",
    "        for split in test_splits:\n",
    "            found = False  # Flag indicating that split was found and filled in one of backtest paths\n",
    "            for path in self.backtest_paths:\n",
    "                for path_el in path:\n",
    "                    if path_el['train'] is None and split == path_el['test'] and found is False:\n",
    "                        path_el['train'] = np.array(train_indices)\n",
    "                        path_el['test'] = list(range(split[0], split[-1]))\n",
    "                        found = True\n",
    "\n",
    "    # noinspection PyPep8Naming\n",
    "    def split(self,\n",
    "              X: pd.DataFrame,\n",
    "              y: pd.Series = None,\n",
    "              groups=None):\n",
    "        \"\"\"\n",
    "        The main method to call for the PurgedKFold class\n",
    "\n",
    "        :param X: (pd.DataFrame) Samples dataset that is to be split\n",
    "        :param y: (pd.Series) Sample labels series\n",
    "        :param groups: (array-like), with shape (n_samples,), optional\n",
    "            Group labels for the samples used while splitting the dataset into\n",
    "            train/test set.\n",
    "        :return: (tuple) [train list of sample indices, and test list of sample indices]\n",
    "        \"\"\"\n",
    "        if X.shape[0] != self.samples_info_sets.shape[0]:\n",
    "            raise ValueError(\"X and the 'samples_info_sets' series param must be the same length\")\n",
    "\n",
    "        test_ranges: [(int, int)] = [(ix[0], ix[-1] + 1) for ix in np.array_split(np.arange(X.shape[0]), self.n_splits)]\n",
    "        print (\"test_ranges: \", test_ranges)\n",
    "        print (\"test_ranges len: \", len(test_ranges))\n",
    "        splits_indices = {}\n",
    "        splits_indices_reverse = {}\n",
    "        for index, [start_ix, end_ix] in enumerate(test_ranges):\n",
    "            splits_indices[index] = [start_ix, end_ix]\n",
    "            splits_indices_reverse[start_ix, end_ix] = [index]\n",
    "        print (\"splits_indices: \", splits_indices)\n",
    "        print (\"splits_indices len : \", len(splits_indices))\n",
    "        print (\"splits_indices_reverse: \", splits_indices_reverse)\n",
    "        combinatorial_test_ranges = self._generate_combinatorial_test_ranges(splits_indices)\n",
    "        \n",
    "        print (\"combinatorial_test_ranges: \", combinatorial_test_ranges)\n",
    "        print (\"combinatorial_test_ranges len: \", len(combinatorial_test_ranges))\n",
    "        \n",
    "        # Prepare backtest paths\n",
    "        for _ in range(self.num_backtest_paths):\n",
    "            path = []\n",
    "            for split_idx in splits_indices.values():\n",
    "                path.append({'train': None, 'test': split_idx})\n",
    "            self.backtest_paths.append(path)\n",
    "\n",
    "        print ('X.shape: ',X.shape)\n",
    "        embargo: int = int(X.shape[0] * self.pct_embargo)\n",
    "        print (\"comb test ranges len: \", len(combinatorial_test_ranges))\n",
    "        for test_splits in combinatorial_test_ranges:\n",
    "            \n",
    "            print (\"current test_splits: \", test_splits)\n",
    "            print (splits_indices_reverse[tuple(test_splits[0])], splits_indices_reverse[tuple(test_splits[1])])\n",
    "            \n",
    "            # print (   )\n",
    "            \n",
    "            # Embargo\n",
    "            test_times = pd.Series(index=[self.samples_info_sets[ix[0]] for ix in test_splits], data=[\n",
    "                self.samples_info_sets[ix[1] - 1] if ix[1] - 1 + embargo >= X.shape[0] else self.samples_info_sets[\n",
    "                    ix[1] - 1 + embargo]\n",
    "                for ix in test_splits]) # test time list 에 train 으로부터 소거할 만큼\n",
    "\n",
    "            test_indices = []\n",
    "            for [start_ix, end_ix] in test_splits:\n",
    "                test_indices.extend(list(range(start_ix, end_ix)))\n",
    "\n",
    "            # Purge\n",
    "            train_times = ml_get_train_times(self.samples_info_sets, test_times)\n",
    "\n",
    "            # Get indices\n",
    "            train_indices = []\n",
    "            for train_ix in train_times.index:\n",
    "                train_indices.append(self.samples_info_sets.index.get_loc(train_ix))\n",
    "\n",
    "            self._fill_backtest_paths(train_indices, test_splits)\n",
    "\n",
    "            yield np.array(train_indices), np.array(test_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "numFold = 6 \n",
    "combNum = 2\n",
    "folds = [i for i in range(numFold)]\n",
    "val_comb = list(combinations(folds, combNum))\n",
    "fold_set = [(ix[0], ix[-1] + 1) for ix in np.array_split(np.arange(TRAIN.shape[0]), 6)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_points = 12\n",
    "sample_info_sets = pd.Series(index=training_data[:-history_points].index, data=training_data[history_points:].index)\n",
    "    # history_points 간격을 유지하면서 진행 \n",
    "\n",
    "pct_embargo = 0.01\n",
    "\n",
    "cv_gen_purged = CombinatorialPurgedKFold(n_splits=6, n_test_splits= 2,samples_info_sets=sample_info_sets, pct_embargo=pct_embargo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0 \n",
    "\n",
    "all_X_training_data = np.array([training_data_array[i:i+history_points].copy() for i in range(len(training_data_array) - history_points)])\n",
    "all_y_training_data = np.array([training_data_array[i + history_points].copy() for i in range(len(training_data_array) - history_points)])\n",
    "gen = cv_gen_purged.split(X=all_X_training_data, y=all_y_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_shape_ls = []\n",
    "valid_shape_ls = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 1\n",
      "test_ranges:  [(0, 1060), (1060, 2120), (2120, 3180), (3180, 4240), (4240, 5300), (5300, 6360)]\n",
      "test_ranges len:  6\n",
      "splits_indices:  {0: [0, 1060], 1: [1060, 2120], 2: [2120, 3180], 3: [3180, 4240], 4: [4240, 5300], 5: [5300, 6360]}\n",
      "splits_indices len :  6\n",
      "splits_indices_reverse:  {(0, 1060): [0], (1060, 2120): [1], (2120, 3180): [2], (3180, 4240): [3], (4240, 5300): [4], (5300, 6360): [5]}\n",
      "combinatorial_test_ranges:  [[[0, 1060], [1060, 2120]], [[0, 1060], [2120, 3180]], [[0, 1060], [3180, 4240]], [[0, 1060], [4240, 5300]], [[0, 1060], [5300, 6360]], [[1060, 2120], [2120, 3180]], [[1060, 2120], [3180, 4240]], [[1060, 2120], [4240, 5300]], [[1060, 2120], [5300, 6360]], [[2120, 3180], [3180, 4240]], [[2120, 3180], [4240, 5300]], [[2120, 3180], [5300, 6360]], [[3180, 4240], [4240, 5300]], [[3180, 4240], [5300, 6360]], [[4240, 5300], [5300, 6360]]]\n",
      "combinatorial_test_ranges len:  15\n",
      "X.shape:  (6360, 12, 16)\n",
      "comb test ranges len:  15\n",
      "current test_splits:  [[0, 1060], [1060, 2120]]\n",
      "[0] [1]\n",
      "\n",
      "\n",
      "train shape:  (4165,) valid shape:  (2120,)\n"
     ]
    }
   ],
   "source": [
    "i +=1\n",
    "print ('i:', i)\n",
    "train, valid = next(gen)\n",
    "print (\"\\n\")\n",
    "print (\"train shape: \", train.shape, \"valid shape: \", valid.shape)\n",
    "train_shape_ls.append(train.shape)\n",
    "valid_shape_ls.append(valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 2\n",
      "current test_splits:  [[0, 1060], [2120, 3180]]\n",
      "[0] [2]\n",
      "\n",
      "\n",
      "train shape:  (4090,) valid shape:  (2120,)\n"
     ]
    }
   ],
   "source": [
    "i +=1\n",
    "print ('i:', i)\n",
    "train, valid = next(gen)\n",
    "print (\"\\n\")\n",
    "print (\"train shape: \", train.shape, \"valid shape: \", valid.shape)\n",
    "train_shape_ls.append(train.shape)\n",
    "valid_shape_ls.append(valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2120\n"
     ]
    }
   ],
   "source": [
    "print ((1060-0)+(6360 - 5300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4165,), (4090,)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_shape_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2120,), (2120,)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_shape_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_dict = {}\n",
    "\n",
    "for key, value in enumerate(fold_set):\n",
    "    fold_dict[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: (0, 852),\n",
       " 1: (852, 1704),\n",
       " 2: (1704, 2555),\n",
       " 3: (2555, 3406),\n",
       " 4: (3406, 4257),\n",
       " 5: (4257, 5108)}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_splits' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_32372\\1601335919.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_splits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'test_splits' is not defined"
     ]
    }
   ],
   "source": [
    "test_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinatorial_test_ranges = cv_gen_purged._generate_combinatorial_test_ranges(fold_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 852), (852, 1704)],\n",
       " [(0, 852), (1704, 2555)],\n",
       " [(0, 852), (2555, 3406)],\n",
       " [(0, 852), (3406, 4257)],\n",
       " [(0, 852), (4257, 5108)],\n",
       " [(852, 1704), (1704, 2555)],\n",
       " [(852, 1704), (2555, 3406)],\n",
       " [(852, 1704), (3406, 4257)],\n",
       " [(852, 1704), (4257, 5108)],\n",
       " [(1704, 2555), (2555, 3406)],\n",
       " [(1704, 2555), (3406, 4257)],\n",
       " [(1704, 2555), (4257, 5108)],\n",
       " [(2555, 3406), (3406, 4257)],\n",
       " [(2555, 3406), (4257, 5108)],\n",
       " [(3406, 4257), (4257, 5108)]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combinatorial_test_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combinatorial_test_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1997-06-05'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_info_sets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date\n",
       "1997-05-20    1997-06-05\n",
       "1997-05-21    1997-06-06\n",
       "1997-05-22    1997-06-09\n",
       "1997-05-23    1997-06-10\n",
       "1997-05-26    1997-06-11\n",
       "                 ...    \n",
       "2021-09-28    2021-10-14\n",
       "2021-09-29    2021-10-15\n",
       "2021-09-30    2021-10-18\n",
       "2021-10-01    2021-10-19\n",
       "2021-10-04    2021-10-20\n",
       "Name: date, Length: 6360, dtype: object"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_info_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_def():\n",
    "    for i in [1,2,3]:\n",
    "        yield (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = yield_def()\n",
    "next(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "\n",
    "train_id = []\n",
    "valid_id = []\n",
    "all_X_training_data = np.array([training_data_array[i:i+history_points].copy() for i in range(len(training_data_array) - history_points)])\n",
    "all_y_training_data = np.array([training_data_array[i + history_points].copy() for i in range(len(training_data_array) - history_points)])\n",
    "gen = cv_gen_purged.split(X=all_X_training_data, y=all_y_training_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "914.6",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\envs\\tft\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3360\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3361\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3362\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tft\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tft\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 914.6",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22900\\3815736781.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m                 sample_info_sets[ix[1] - 1] if ix[1] - 1 + embargo >= all_X_training_data.shape[0] else sample_info_sets[\n\u001b[0;32m      7\u001b[0m                     ix[1] - 1 + embargo]\n\u001b[1;32m----> 8\u001b[1;33m                 for ix in test_splits])\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22900\\3815736781.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      6\u001b[0m                 sample_info_sets[ix[1] - 1] if ix[1] - 1 + embargo >= all_X_training_data.shape[0] else sample_info_sets[\n\u001b[0;32m      7\u001b[0m                     ix[1] - 1 + embargo]\n\u001b[1;32m----> 8\u001b[1;33m                 for ix in test_splits])\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\tft\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    940\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    941\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 942\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    943\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    944\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tft\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m_get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1049\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1050\u001b[0m         \u001b[1;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1051\u001b[1;33m         \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_values_for_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tft\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3361\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3362\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3363\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3365\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 914.6"
     ]
    }
   ],
   "source": [
    "embargo = all_X_training_data.shape[0] * pct_embargo\n",
    "\n",
    "test_splits = fold_set\n",
    "\n",
    "test_times = pd.Series(index=[sample_info_sets[ix[0]] for ix in test_splits], data=[\n",
    "                sample_info_sets[ix[1] - 1] if ix[1] - 1 + embargo >= all_X_training_data.shape[0] else sample_info_sets[\n",
    "                    ix[1] - 1 + embargo]\n",
    "                for ix in test_splits])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_points = 12\n",
    "sample_info_sets = pd.Series(index=training_data[:-history_points].index, data=training_data[history_points:].index)\n",
    "\n",
    "pct_embargo = 0.01\n",
    "cv_gen_purged = CB.CombinatorialPurgedKFold(n_splits=4, samples_info_sets=sample_info_sets, pct_embargo=pct_embargo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6385, 16)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6360,)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_info_sets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_gen_purged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 852), (852, 1704)],\n",
       " [(0, 852), (1704, 2555)],\n",
       " [(0, 852), (2555, 3406)],\n",
       " [(0, 852), (3406, 4257)],\n",
       " [(0, 852), (4257, 5108)],\n",
       " [(852, 1704), (1704, 2555)],\n",
       " [(852, 1704), (2555, 3406)],\n",
       " [(852, 1704), (3406, 4257)],\n",
       " [(852, 1704), (4257, 5108)],\n",
       " [(1704, 2555), (2555, 3406)],\n",
       " [(1704, 2555), (3406, 4257)],\n",
       " [(1704, 2555), (4257, 5108)],\n",
       " [(2555, 3406), (3406, 4257)],\n",
       " [(2555, 3406), (4257, 5108)],\n",
       " [(3406, 4257), (4257, 5108)]]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combinatorial_test_ranges"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "26b462196a458278e1b93f4dafd0c6a4f17b941fc39fbc23177d70dd3e3b7653"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('tft')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
