{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import datetime \n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from CV import cross_validation as CV\n",
    "from CV import combinatorial as CB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Order \n",
    "1. X, Y generation \n",
    "2. CPCV baseline \n",
    "   어차피 여기서 뒤에서 개수만큼 잘라주는 거면 데이터 포인트를 자르는 것과 다를바 없음\n",
    "3. Path return function\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/data_input_demo.csv\", index_col = [0])\n",
    "df = df.set_index(['date'])[['13ty_index', 'interty_index', 'lty_index', 'mbs_index',\\\n",
    "       '13cy_index', 'intercy_index', 'lcy_index', 'ty_index', 'cy_index','agg_index']]\n",
    "df.index = pd.to_datetime(df.index, format = '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>13ty_index</th>\n",
       "      <th>interty_index</th>\n",
       "      <th>lty_index</th>\n",
       "      <th>mbs_index</th>\n",
       "      <th>13cy_index</th>\n",
       "      <th>intercy_index</th>\n",
       "      <th>lcy_index</th>\n",
       "      <th>ty_index</th>\n",
       "      <th>cy_index</th>\n",
       "      <th>agg_index</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1997-05-19</th>\n",
       "      <td>133.46</td>\n",
       "      <td>784.22</td>\n",
       "      <td>840.64</td>\n",
       "      <td>751.37</td>\n",
       "      <td>668.10</td>\n",
       "      <td>893.58</td>\n",
       "      <td>912.47</td>\n",
       "      <td>824.23</td>\n",
       "      <td>861.00</td>\n",
       "      <td>715.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-05-20</th>\n",
       "      <td>133.58</td>\n",
       "      <td>785.01</td>\n",
       "      <td>840.34</td>\n",
       "      <td>751.82</td>\n",
       "      <td>668.74</td>\n",
       "      <td>894.56</td>\n",
       "      <td>912.29</td>\n",
       "      <td>824.78</td>\n",
       "      <td>861.51</td>\n",
       "      <td>716.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-05-21</th>\n",
       "      <td>133.59</td>\n",
       "      <td>784.63</td>\n",
       "      <td>836.47</td>\n",
       "      <td>751.97</td>\n",
       "      <td>668.77</td>\n",
       "      <td>893.58</td>\n",
       "      <td>908.76</td>\n",
       "      <td>823.54</td>\n",
       "      <td>859.71</td>\n",
       "      <td>715.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-05-22</th>\n",
       "      <td>133.58</td>\n",
       "      <td>784.33</td>\n",
       "      <td>834.66</td>\n",
       "      <td>751.97</td>\n",
       "      <td>668.71</td>\n",
       "      <td>892.96</td>\n",
       "      <td>906.86</td>\n",
       "      <td>822.86</td>\n",
       "      <td>858.69</td>\n",
       "      <td>714.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-05-23</th>\n",
       "      <td>133.58</td>\n",
       "      <td>784.70</td>\n",
       "      <td>835.59</td>\n",
       "      <td>752.27</td>\n",
       "      <td>668.71</td>\n",
       "      <td>893.58</td>\n",
       "      <td>907.77</td>\n",
       "      <td>823.37</td>\n",
       "      <td>859.37</td>\n",
       "      <td>715.31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            13ty_index  interty_index  lty_index  mbs_index  13cy_index  \\\n",
       "date                                                                      \n",
       "1997-05-19      133.46         784.22     840.64     751.37      668.10   \n",
       "1997-05-20      133.58         785.01     840.34     751.82      668.74   \n",
       "1997-05-21      133.59         784.63     836.47     751.97      668.77   \n",
       "1997-05-22      133.58         784.33     834.66     751.97      668.71   \n",
       "1997-05-23      133.58         784.70     835.59     752.27      668.71   \n",
       "\n",
       "            intercy_index  lcy_index  ty_index  cy_index  agg_index  \n",
       "date                                                                 \n",
       "1997-05-19         893.58     912.47    824.23    861.00     715.66  \n",
       "1997-05-20         894.56     912.29    824.78    861.51     716.09  \n",
       "1997-05-21         893.58     908.76    823.54    859.71     715.31  \n",
       "1997-05-22         892.96     906.86    822.86    858.69     714.88  \n",
       "1997-05-23         893.58     907.77    823.37    859.37     715.31  "
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X, Y generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_xy_seq(df: pd.DataFrame, x_seq = 66, y_seq = 22):\n",
    "    \"\"\"\n",
    "    Generate samples from\n",
    "    :param df:\n",
    "    :param x_seq:\n",
    "    :param y_seq:\n",
    "    :param scaler:\n",
    "    :return:\n",
    "    # x: (epoch_size, input_length, num_nodes, input_dim)\n",
    "    # y: (epoch_size, output_length, num_nodes, output_dim)\n",
    "    \"\"\"\n",
    "    num_samples, num_nodes = df.shape\n",
    "    dates_arr = np.array(df.index)\n",
    "    data = np.expand_dims(df.values, axis = -1) # df -> array [N, F, 1]\n",
    "\n",
    "    x_offsets = np.arange(-x_seq+1, 1)  \n",
    "    y_offsets = np.arange(1, y_seq+1)\n",
    "\n",
    "    # feature_list = [data]\n",
    "\n",
    "    x, y = [], []\n",
    "    x_date, y_date = [],[]\n",
    "\n",
    "    min_t = abs(min(x_offsets))\n",
    "    max_t = abs(num_samples - abs(max(y_offsets)))\n",
    "\n",
    "    for t in range(min_t, max_t):\n",
    "        # value seperation\n",
    "        x.append(data[t+x_offsets, ...])\n",
    "        y.append(data[t+y_offsets, ...])\n",
    "        # date seperation\n",
    "        x_date.append(dates_arr[t+x_offsets])\n",
    "        y_date.append(dates_arr[t+y_offsets])\n",
    "        \n",
    "    x = np.squeeze(np.stack(x, axis = 0))\n",
    "    y = np.squeeze(np.stack(y, axis = 0))\n",
    "\n",
    "    x_date = np.stack(x_date, axis = 0)\n",
    "    y_date = np.stack(y_date, axis = 0)\n",
    "\n",
    "    return x, y, x_date, y_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_seq = 66\n",
    "future_seq = 22\n",
    "num_assets = df.shape[1]\n",
    "X, Y, X_date, Y_date = generate_xy_seq(df, x_seq = past_seq, y_seq = future_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6298, 66, 10)"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6298, 22, 10)"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6298, 66, 10)"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['1997-05-19T00:00:00.000000000', '1997-05-20T00:00:00.000000000',\n",
       "        '1997-05-21T00:00:00.000000000', ...,\n",
       "        '1997-08-14T00:00:00.000000000', '1997-08-15T00:00:00.000000000',\n",
       "        '1997-08-18T00:00:00.000000000'],\n",
       "       ['1997-05-20T00:00:00.000000000', '1997-05-21T00:00:00.000000000',\n",
       "        '1997-05-22T00:00:00.000000000', ...,\n",
       "        '1997-08-15T00:00:00.000000000', '1997-08-18T00:00:00.000000000',\n",
       "        '1997-08-19T00:00:00.000000000'],\n",
       "       ['1997-05-21T00:00:00.000000000', '1997-05-22T00:00:00.000000000',\n",
       "        '1997-05-23T00:00:00.000000000', ...,\n",
       "        '1997-08-18T00:00:00.000000000', '1997-08-19T00:00:00.000000000',\n",
       "        '1997-08-20T00:00:00.000000000'],\n",
       "       ...,\n",
       "       ['2021-07-05T00:00:00.000000000', '2021-07-06T00:00:00.000000000',\n",
       "        '2021-07-07T00:00:00.000000000', ...,\n",
       "        '2021-09-30T00:00:00.000000000', '2021-10-01T00:00:00.000000000',\n",
       "        '2021-10-04T00:00:00.000000000'],\n",
       "       ['2021-07-06T00:00:00.000000000', '2021-07-07T00:00:00.000000000',\n",
       "        '2021-07-08T00:00:00.000000000', ...,\n",
       "        '2021-10-01T00:00:00.000000000', '2021-10-04T00:00:00.000000000',\n",
       "        '2021-10-05T00:00:00.000000000'],\n",
       "       ['2021-07-07T00:00:00.000000000', '2021-07-08T00:00:00.000000000',\n",
       "        '2021-07-09T00:00:00.000000000', ...,\n",
       "        '2021-10-04T00:00:00.000000000', '2021-10-05T00:00:00.000000000',\n",
       "        '2021-10-06T00:00:00.000000000']], dtype='datetime64[ns]')"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['1997-08-19T00:00:00.000000000', '1997-08-20T00:00:00.000000000',\n",
       "        '1997-08-21T00:00:00.000000000', ...,\n",
       "        '1997-09-15T00:00:00.000000000', '1997-09-16T00:00:00.000000000',\n",
       "        '1997-09-17T00:00:00.000000000'],\n",
       "       ['1997-08-20T00:00:00.000000000', '1997-08-21T00:00:00.000000000',\n",
       "        '1997-08-22T00:00:00.000000000', ...,\n",
       "        '1997-09-16T00:00:00.000000000', '1997-09-17T00:00:00.000000000',\n",
       "        '1997-09-18T00:00:00.000000000'],\n",
       "       ['1997-08-21T00:00:00.000000000', '1997-08-22T00:00:00.000000000',\n",
       "        '1997-08-25T00:00:00.000000000', ...,\n",
       "        '1997-09-17T00:00:00.000000000', '1997-09-18T00:00:00.000000000',\n",
       "        '1997-09-19T00:00:00.000000000'],\n",
       "       ...,\n",
       "       ['2021-10-05T00:00:00.000000000', '2021-10-06T00:00:00.000000000',\n",
       "        '2021-10-07T00:00:00.000000000', ...,\n",
       "        '2021-11-01T00:00:00.000000000', '2021-11-02T00:00:00.000000000',\n",
       "        '2021-11-03T00:00:00.000000000'],\n",
       "       ['2021-10-06T00:00:00.000000000', '2021-10-07T00:00:00.000000000',\n",
       "        '2021-10-08T00:00:00.000000000', ...,\n",
       "        '2021-11-02T00:00:00.000000000', '2021-11-03T00:00:00.000000000',\n",
       "        '2021-11-04T00:00:00.000000000'],\n",
       "       ['2021-10-07T00:00:00.000000000', '2021-10-08T00:00:00.000000000',\n",
       "        '2021-10-11T00:00:00.000000000', ...,\n",
       "        '2021-11-03T00:00:00.000000000', '2021-11-04T00:00:00.000000000',\n",
       "        '2021-11-05T00:00:00.000000000']], dtype='datetime64[ns]')"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_info_sets = pd.Series(index=df[:-(past_seq+future_seq-1)].index, data=df[(past_seq+future_seq-1):].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date\n",
       "1997-05-19   1997-09-17\n",
       "1997-05-20   1997-09-18\n",
       "1997-05-21   1997-09-19\n",
       "1997-05-22   1997-09-22\n",
       "1997-05-23   1997-09-23\n",
       "                ...    \n",
       "2021-07-01   2021-11-01\n",
       "2021-07-02   2021-11-02\n",
       "2021-07-05   2021-11-03\n",
       "2021-07-06   2021-11-04\n",
       "2021-07-07   2021-11-05\n",
       "Name: date, Length: 6298, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_info_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Implements the Combinatorial Purged Cross-Validation class from Chapter 12\n",
    "\"\"\"\n",
    "import sys \n",
    "\n",
    "from itertools import combinations\n",
    "from typing import List\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy.special import comb\n",
    "from sklearn.model_selection import KFold\n",
    "from CV.cross_validation import ml_get_train_times\n",
    "\n",
    "def _get_number_of_backtest_paths(n_train_splits: int, n_test_splits: int) -> float:\n",
    "    \"\"\"\n",
    "    Number of combinatorial paths for CPCV(N,K)\n",
    "    :param n_train_splits: (int) number of train splits\n",
    "    :param n_test_splits: (int) number of test splits\n",
    "    :return: (int) number of backtest paths for CPCV(N,k)\n",
    "    \"\"\"\n",
    "    return int(comb(n_train_splits, n_train_splits - n_test_splits) * n_test_splits / n_train_splits)\n",
    "\n",
    "\n",
    "class CombinatorialPurgedKFold(KFold):\n",
    "    \"\"\"\n",
    "    Advances in Financial Machine Learning, Chapter 12.\n",
    "\n",
    "    Implements Combinatial Purged Cross Validation (CPCV)\n",
    "\n",
    "    The train is purged of observations overlapping test-label intervals\n",
    "    Test set is assumed contiguous (shuffle=False), w/o training samples in between\n",
    "\n",
    "    :param n_splits: (int) The number of splits. Default to 3\n",
    "    :param samples_info_sets: (pd.Series, date value) The information range on which each record is constructed from\n",
    "        *samples_info_sets.index*: Time when the information extraction started.\n",
    "        *samples_info_sets.value*: Time when the information extraction ended.\n",
    "    :param pct_embargo: (float) Percent that determines the embargo size.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_splits: int = 3,\n",
    "                 n_test_splits: int = 2,\n",
    "                 samples_info_sets: pd.Series = None,\n",
    "                 pct_embargo: float = 0.,\n",
    "                 purge_behind: bool = False):\n",
    "\n",
    "        if not isinstance(samples_info_sets, pd.Series):\n",
    "            raise ValueError('The samples_info_sets param must be a pd.Series')\n",
    "        super(CombinatorialPurgedKFold, self).__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.samples_info_sets = samples_info_sets\n",
    "        self.pct_embargo = pct_embargo\n",
    "        self.n_test_splits = n_test_splits\n",
    "        self.num_backtest_paths = _get_number_of_backtest_paths(self.n_splits, self.n_test_splits) # self. 정의가 안 되어 있는데/./? \n",
    "        self.backtest_paths = []  # Array of backtest paths\n",
    "        self.valid_comb = list(combinations([x for x in range(self.n_splits)], self.n_test_splits))\n",
    "        self._set_path_indexs() # train 별 path index mapping \n",
    "        self.purge_behind = purge_behind\n",
    "        \n",
    "    def _set_path_indexs(self):\n",
    "        train_path_count = [0 for _ in range(self.n_splits)]\n",
    "        self.val_path_pair = {}\n",
    "        for val_group in self.valid_comb: \n",
    "            path_comb = []\n",
    "            for split_ind in val_group:\n",
    "                path_comb.append(train_path_count[split_ind])\n",
    "                train_path_count[split_ind] += 1\n",
    "            self.val_path_pair[val_group] = tuple(path_comb)\n",
    "        \n",
    "\n",
    "    def _generate_combinatorial_test_ranges(self, splits_indices: dict):\n",
    "        \"\"\"\n",
    "        Using start and end indices of test splits from KFolds and number of test_splits (self.n_test_splits),\n",
    "        generates combinatorial test ranges splits\n",
    "\n",
    "        :param splits_indices: (dict) Test fold integer index: [start test index, end test index]\n",
    "        :return: (list) Combinatorial test splits ([start index, end index])\n",
    "        \"\"\"\n",
    "\n",
    "        # Possible test splits for each fold\n",
    "        combinatorial_splits = list(combinations(list(splits_indices.keys()), self.n_test_splits))\n",
    "        combinatorial_test_ranges = []  # List of test indices formed from combinatorial splits\n",
    "        for combination in combinatorial_splits:\n",
    "            temp_test_indices = []  # Array of test indices for current split combination\n",
    "            for int_index in combination:\n",
    "                temp_test_indices.append(splits_indices[int_index])\n",
    "            combinatorial_test_ranges.append(temp_test_indices)\n",
    "        return combinatorial_test_ranges\n",
    "\n",
    "    def _fill_backtest_paths(self, train_indices: list, test_splits: list):\n",
    "        \"\"\"\n",
    "        Using start and end indices of test splits and purged/embargoed train indices from CPCV, find backtest path and\n",
    "        place in the path where these indices should be used.\n",
    "\n",
    "        :param test_splits: (list) of lists with first element corresponding to test start index and second - test end\n",
    "        \"\"\"\n",
    "        # Fill backtest paths using train/test splits from CPCV\n",
    "        for split in test_splits:\n",
    "            found = False  # Flag indicating that split was found and filled in one of backtest paths\n",
    "            for path in self.backtest_paths:\n",
    "                for path_el in path:\n",
    "                    if path_el['train'] is None and split == path_el['test'] and found is False:\n",
    "                        path_el['train'] = np.array(train_indices)\n",
    "                        path_el['test'] = list(range(split[0], split[-1]))\n",
    "                        found = True\n",
    "\n",
    "    # noinspection PyPep8Naming\n",
    "    def split(self,\n",
    "              X: pd.DataFrame,\n",
    "              y: pd.Series = None,\n",
    "              groups=None):\n",
    "        \"\"\"\n",
    "        The main method to call for the PurgedKFold class\n",
    "\n",
    "        :param X: (pd.DataFrame) Samples dataset that is to be split\n",
    "        :param y: (pd.Series) Sample labels series\n",
    "        :param groups: (array-like), with shape (n_samples,), optional\n",
    "            Group labels for the samples used while splitting the dataset into\n",
    "            train/test set.\n",
    "        :return: (tuple) [train list of sample indices, and test list of sample indices]\n",
    "        \"\"\"\n",
    "        if X.shape[0] != self.samples_info_sets.shape[0]:\n",
    "            raise ValueError(\"X and the 'samples_info_sets' series param must be the same length\")\n",
    "\n",
    "        test_ranges: [(int, int)] = [(ix[0], ix[-1] + 1) for ix in np.array_split(np.arange(X.shape[0]), self.n_splits)]\n",
    "            # test_ranges : (st, end ind) for each split in the fold (e.g, fold set)\n",
    "               \n",
    "        \n",
    "        splits_indices = {}\n",
    "        for index, [start_ix, end_ix] in enumerate(test_ranges):\n",
    "            splits_indices[index] = [start_ix, end_ix]\n",
    "\n",
    "        combinatorial_test_ranges = self._generate_combinatorial_test_ranges(splits_indices)\n",
    "        \n",
    "        valid_order_comb_map = {} # 0: (0,1), \n",
    "        for i in range(len(self.valid_comb)):\n",
    "            valid_order_comb_map[i] = self.valid_comb[i]\n",
    "        \n",
    "        spilt_ind_map = {}\n",
    "        for i in range(len(self.valid_comb)): \n",
    "            spilt_ind_map[i] = self.valid_comb[i] \n",
    "        \n",
    "        # Prepare backtest paths\n",
    "        for _ in range(self.num_backtest_paths):\n",
    "            path = []\n",
    "            for split_idx in splits_indices.values():\n",
    "                path.append({'train': None, 'test': split_idx})\n",
    "            self.backtest_paths.append(path)\n",
    "\n",
    "        embargo: int = int(X.shape[0] * self.pct_embargo)\n",
    "        self.embargo = embargo\n",
    "        \n",
    "        for comb_order, test_splits in enumerate(combinatorial_test_ranges):\n",
    "                # test_splits : [[0,10], [10, 21], [21, 31]]  test set 만 가지고 있음 \n",
    "            \n",
    "            if self.purge_behind:\n",
    "                cv_set = {}\n",
    "                val_comb_ind_set = valid_order_comb_map[comb_order]\n",
    "                valid_set = {}\n",
    "                train_set = {}\n",
    "                \n",
    "                for split_ind, _ in enumerate(test_ranges):\n",
    "                    if split_ind in val_comb_ind_set: # validation\n",
    "                        split = test_ranges[split_ind]\n",
    "                        for st_ind, end_ind in [split]:\n",
    "                            valid_ind = np.arange(st_ind,end_ind)\n",
    "                            valid_set[split_ind] = (valid_ind)\n",
    "                    else: # train\n",
    "                        split = test_ranges[split_ind]\n",
    "                        for st_ind, end_ind in [split]:\n",
    "                            train_ind = np.arange(st_ind, end_ind)\n",
    "                            train_set[split_ind] = (train_ind)\n",
    "                cv_set['val'] = valid_set\n",
    "                cv_set['train'] = train_set    \n",
    "                \n",
    "                # Purging & Embargo \n",
    "                print (cv_set)\n",
    "                \n",
    "                train_indices, test_indices = self.purging(cv_set)\n",
    "            else:\n",
    "                # Embargo\n",
    "                test_times = pd.Series(index=[self.samples_info_sets[ix[0]] for ix in test_splits], data=[\n",
    "                    self.samples_info_sets[ix[1] - 1] if ix[1] - 1 + embargo >= X.shape[0] else self.samples_info_sets[\n",
    "                        ix[1] - 1 + embargo]\n",
    "                    for ix in test_splits])\n",
    "\n",
    "                test_indices = []\n",
    "                for [start_ix, end_ix] in test_splits:\n",
    "                    test_indices.append(list(range(start_ix, end_ix)))\n",
    "\n",
    "                # Purge\n",
    "                train_times = ml_get_train_times(self.samples_info_sets, test_times)\n",
    "\n",
    "                # Get indices\n",
    "                train_indices = []\n",
    "                for train_ix in train_times.index:\n",
    "                    train_indices.append(self.samples_info_sets.index.get_loc(train_ix))\n",
    "            \n",
    "            valid_group_indices = spilt_ind_map[comb_order]\n",
    "            path_ls = self.val_path_pair[valid_group_indices] # path_ls : [path num1, path num2,..path numN]\n",
    "\n",
    "            self._fill_backtest_paths(train_indices, test_splits)\n",
    "\n",
    "            yield path_ls, np.array(train_indices), [np.array(x) for x in test_indices]\n",
    "\n",
    "    def purging(self, cv_set):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            cv_set (double nested dictionary)\n",
    "                - keys : ['train', 'val']\n",
    "                - value : 2nd dict\n",
    "                    - keys: fold num [0, 1]\n",
    "                    - values (np) \n",
    "        Return: \n",
    "            purged train or validation data set\n",
    "        \"\"\"\n",
    "        purging_num = self.embargo\n",
    "        embargo_num = self.embargo\n",
    "\n",
    "        val_keys = list(cv_set['val'].keys())\n",
    "        train_keys = list(cv_set['train'].keys())\n",
    "        # print (\"val_keys \", val_keys)\n",
    " \n",
    "        for val_k in val_keys:\n",
    "            # print (val_k)\n",
    "            if val_k-1 in train_keys: # train | val -> val 앞 버리기 \n",
    "                # print (\" o | x \")\n",
    "                cv_set['val'][val_k] = cv_set['val'][val_k][purging_num: ]\n",
    "                if val_k+1 in train_keys: # val | train -> train 앞 버리기 \n",
    "                    # print (\" x | o \")\n",
    "                    cv_set['train'][val_k+1] = cv_set['train'][val_k+1][embargo_num: ]\n",
    "\n",
    "            elif val_k+1 in train_keys: # val | train -> train 앞 버리기 \n",
    "                # print (\" x | o \")\n",
    "                cv_set['train'][val_k+1] = cv_set['train'][val_k+1][embargo_num: ]\n",
    "                if val_k-1 in train_keys: # train | val -> val 앞 버리기 \n",
    "                    # print (\" o | x | \")\n",
    "                    cv_set['val'][val_k] = cv_set['val'][val_k][purging_num: ]\n",
    "        \n",
    "        train_indices = [x for y in list(cv_set['train'].values()) for x in y]\n",
    "        test_indices = cv_set['val'].values()\n",
    "\n",
    "        return train_indices, test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_split_num = 6\n",
    "val_split_num = 2\n",
    "pct_embargo = 0.01\n",
    "\n",
    "folds = [i for i in range(total_split_num)]\n",
    "val_comb = list(combinations(folds, val_split_num))\n",
    "train_split_num = total_split_num - val_split_num\n",
    "path_fold_num  = train_split_num + 1 # 한 path 에 존재하는 fold 의 개수 / train_split_num + 1  = 5\n",
    "path_num = int(len(val_comb) * val_split_num / total_split_num) # 전체 path 의 개수 = path_fold_num \n",
    "\n",
    "cv_gen = CombinatorialPurgedKFold(n_splits=total_split_num,\n",
    "                         n_test_splits = val_split_num, \n",
    "                         samples_info_sets=sample_info_sets, \n",
    "                         pct_embargo=pct_embargo)\n",
    "\n",
    "cv_split = cv_gen.split(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>13ty_index</th>\n",
       "      <th>interty_index</th>\n",
       "      <th>lty_index</th>\n",
       "      <th>mbs_index</th>\n",
       "      <th>13cy_index</th>\n",
       "      <th>intercy_index</th>\n",
       "      <th>lcy_index</th>\n",
       "      <th>ty_index</th>\n",
       "      <th>cy_index</th>\n",
       "      <th>agg_index</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1997-05-19</th>\n",
       "      <td>133.46</td>\n",
       "      <td>784.22</td>\n",
       "      <td>840.64</td>\n",
       "      <td>751.37</td>\n",
       "      <td>668.10</td>\n",
       "      <td>893.58</td>\n",
       "      <td>912.47</td>\n",
       "      <td>824.23</td>\n",
       "      <td>861.00</td>\n",
       "      <td>715.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-05-20</th>\n",
       "      <td>133.58</td>\n",
       "      <td>785.01</td>\n",
       "      <td>840.34</td>\n",
       "      <td>751.82</td>\n",
       "      <td>668.74</td>\n",
       "      <td>894.56</td>\n",
       "      <td>912.29</td>\n",
       "      <td>824.78</td>\n",
       "      <td>861.51</td>\n",
       "      <td>716.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-05-21</th>\n",
       "      <td>133.59</td>\n",
       "      <td>784.63</td>\n",
       "      <td>836.47</td>\n",
       "      <td>751.97</td>\n",
       "      <td>668.77</td>\n",
       "      <td>893.58</td>\n",
       "      <td>908.76</td>\n",
       "      <td>823.54</td>\n",
       "      <td>859.71</td>\n",
       "      <td>715.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-05-22</th>\n",
       "      <td>133.58</td>\n",
       "      <td>784.33</td>\n",
       "      <td>834.66</td>\n",
       "      <td>751.97</td>\n",
       "      <td>668.71</td>\n",
       "      <td>892.96</td>\n",
       "      <td>906.86</td>\n",
       "      <td>822.86</td>\n",
       "      <td>858.69</td>\n",
       "      <td>714.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-05-23</th>\n",
       "      <td>133.58</td>\n",
       "      <td>784.70</td>\n",
       "      <td>835.59</td>\n",
       "      <td>752.27</td>\n",
       "      <td>668.71</td>\n",
       "      <td>893.58</td>\n",
       "      <td>907.77</td>\n",
       "      <td>823.37</td>\n",
       "      <td>859.37</td>\n",
       "      <td>715.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-01</th>\n",
       "      <td>277.95</td>\n",
       "      <td>2050.97</td>\n",
       "      <td>4522.48</td>\n",
       "      <td>2303.94</td>\n",
       "      <td>1728.81</td>\n",
       "      <td>3090.09</td>\n",
       "      <td>5252.00</td>\n",
       "      <td>2491.10</td>\n",
       "      <td>3441.88</td>\n",
       "      <td>2351.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-02</th>\n",
       "      <td>278.22</td>\n",
       "      <td>2054.41</td>\n",
       "      <td>4540.20</td>\n",
       "      <td>2304.88</td>\n",
       "      <td>1730.46</td>\n",
       "      <td>3095.17</td>\n",
       "      <td>5270.93</td>\n",
       "      <td>2496.45</td>\n",
       "      <td>3450.09</td>\n",
       "      <td>2355.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-03</th>\n",
       "      <td>278.18</td>\n",
       "      <td>2052.47</td>\n",
       "      <td>4498.21</td>\n",
       "      <td>2303.43</td>\n",
       "      <td>1730.23</td>\n",
       "      <td>3092.39</td>\n",
       "      <td>5245.17</td>\n",
       "      <td>2489.74</td>\n",
       "      <td>3441.78</td>\n",
       "      <td>2351.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-04</th>\n",
       "      <td>278.42</td>\n",
       "      <td>2057.49</td>\n",
       "      <td>4542.00</td>\n",
       "      <td>2307.00</td>\n",
       "      <td>1731.71</td>\n",
       "      <td>3100.85</td>\n",
       "      <td>5286.45</td>\n",
       "      <td>2499.61</td>\n",
       "      <td>3457.87</td>\n",
       "      <td>2359.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-05</th>\n",
       "      <td>278.50</td>\n",
       "      <td>2061.28</td>\n",
       "      <td>4603.89</td>\n",
       "      <td>2312.57</td>\n",
       "      <td>1732.42</td>\n",
       "      <td>3108.59</td>\n",
       "      <td>5346.42</td>\n",
       "      <td>2510.41</td>\n",
       "      <td>3478.10</td>\n",
       "      <td>2369.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6385 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            13ty_index  interty_index  lty_index  mbs_index  13cy_index  \\\n",
       "date                                                                      \n",
       "1997-05-19      133.46         784.22     840.64     751.37      668.10   \n",
       "1997-05-20      133.58         785.01     840.34     751.82      668.74   \n",
       "1997-05-21      133.59         784.63     836.47     751.97      668.77   \n",
       "1997-05-22      133.58         784.33     834.66     751.97      668.71   \n",
       "1997-05-23      133.58         784.70     835.59     752.27      668.71   \n",
       "...                ...            ...        ...        ...         ...   \n",
       "2021-11-01      277.95        2050.97    4522.48    2303.94     1728.81   \n",
       "2021-11-02      278.22        2054.41    4540.20    2304.88     1730.46   \n",
       "2021-11-03      278.18        2052.47    4498.21    2303.43     1730.23   \n",
       "2021-11-04      278.42        2057.49    4542.00    2307.00     1731.71   \n",
       "2021-11-05      278.50        2061.28    4603.89    2312.57     1732.42   \n",
       "\n",
       "            intercy_index  lcy_index  ty_index  cy_index  agg_index  \n",
       "date                                                                 \n",
       "1997-05-19         893.58     912.47    824.23    861.00     715.66  \n",
       "1997-05-20         894.56     912.29    824.78    861.51     716.09  \n",
       "1997-05-21         893.58     908.76    823.54    859.71     715.31  \n",
       "1997-05-22         892.96     906.86    822.86    858.69     714.88  \n",
       "1997-05-23         893.58     907.77    823.37    859.37     715.31  \n",
       "...                   ...        ...       ...       ...        ...  \n",
       "2021-11-01        3090.09    5252.00   2491.10   3441.88    2351.92  \n",
       "2021-11-02        3095.17    5270.93   2496.45   3450.09    2355.94  \n",
       "2021-11-03        3092.39    5245.17   2489.74   3441.78    2351.25  \n",
       "2021-11-04        3100.85    5286.45   2499.61   3457.87    2359.40  \n",
       "2021-11-05        3108.59    5346.42   2510.41   3478.10    2369.30  \n",
       "\n",
       "[6385 rows x 10 columns]"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPCV with Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch \n",
    "from torch import nn \n",
    "\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6298, 66, 10)"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Model(nn.Module):\n",
    "    def __init__(self, num_assets, past_seq, future_seq, num_layers, device):\n",
    "        '''\n",
    "        '''\n",
    "        super(LSTM_Model, self).__init__()\n",
    "        self.device = device\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm_in_dim = num_assets\n",
    "        self.lstm_out_dim = num_assets \n",
    "        \n",
    "        self.fc_in_dim = past_seq\n",
    "        self.fc_out_dim = future_seq\n",
    "\n",
    "## LSTM 정의 \n",
    "        self.lstm = nn.LSTM(self.lstm_in_dim, self.lstm_out_dim, num_layers = self.num_layers, batch_first = True)\n",
    "        self.fc = nn.Conv2d(self.fc_in_dim, self.fc_out_dim, kernel_size= (1,1))        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x: [batch size, past sequence]\n",
    "        '''\n",
    "        batch_size = x.size(0)\n",
    "## 초기 hidden state, cell state 정의해주기 : 0으로 준다\n",
    "        hidden_init = torch.zeros(self.num_layers, batch_size, self.lstm_out_dim).to(self.device)\n",
    "        cell_init = torch.zeros(self.num_layers, batch_size, self.lstm_out_dim).to(self.device)\n",
    "        \n",
    "        output, hidden = self.lstm(x, (hidden_init, cell_init))\n",
    "        prediction = output.unsqueeze(-1)\n",
    "        \n",
    "####### 예측길이 바꿔주고 싶을 때 실행        \n",
    "        prediction = self.fc(prediction)\n",
    "        prediction = prediction.squeeze()\n",
    "        \n",
    "        return prediction         \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use gpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print (\"Use gpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print (\"Use cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Model Loading **\n"
     ]
    }
   ],
   "source": [
    "# Model related hyper-parameters \n",
    "num_assets = df.shape[-1]\n",
    "num_layers = 1 # 4 개의 RNN layer 을 쌓겠습니다. \n",
    "num_epochs = 100 # Epoch 횟수를 설정합니다. \n",
    "\n",
    "######### Model 을 정의하는 부분 ########\n",
    "print (\"** Model Loading **\")\n",
    "Model = LSTM_Model(num_assets, past_seq, future_seq, num_layers, device)\n",
    "\n",
    "Model.to(device)\n",
    "\n",
    "criterion = nn.L1Loss() # Loss function 을 정의합니다 : MAE\n",
    "optimizer = torch.optim.Adam(Model.parameters(), lr = 0.01) # optimizer 을 설정해 줍니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training related hyper-parameters\n",
    "batch_num = 32\n",
    "epoch_num = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_split_num = 6\n",
    "val_split_num = 2\n",
    "pct_embargo = 0.01\n",
    "purge_behind = True\n",
    "\n",
    "\n",
    "folds = [i for i in range(total_split_num)]\n",
    "val_comb = list(combinations(folds, val_split_num))\n",
    "train_split_num = total_split_num - val_split_num\n",
    "path_fold_num  = train_split_num + 1 # 한 path 에 존재하는 fold 의 개수 / train_split_num + 1  = 5\n",
    "path_num = int(len(val_comb) * val_split_num / total_split_num) # 전체 path 의 개수 = path_fold_num \n",
    "\n",
    "cv_gen = CombinatorialPurgedKFold(n_splits=total_split_num,\n",
    "                         n_test_splits = val_split_num, \n",
    "                         samples_info_sets=sample_info_sets, \n",
    "                         pct_embargo=pct_embargo,\n",
    "                         purge_behind= purge_behind)\n",
    "\n",
    "cv_split = cv_gen.split(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataLoader(object):\n",
    "    def __init__(self, xs, ys, x_date, y_date, batch_size, device, shuffle = True, pad_with_last_sample=True):\n",
    "        \"\"\"\n",
    "        :param xs:\n",
    "        :param ys:\n",
    "        :param batch_size:\n",
    "        :param pad_with_last_sample: pad with the last sample to make number of samples divisible to batch_size.\n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.current_ind = 0\n",
    "        if pad_with_last_sample:\n",
    "            num_padding = (batch_size - (len(xs) % batch_size)) % batch_size\n",
    "            x_padding = np.repeat(xs[-1:], num_padding, axis=0) # padding 하고자 하는 만큼 뒤에 추가 \n",
    "            y_padding = np.repeat(ys[-1:], num_padding, axis=0)\n",
    "            \n",
    "            x_date_padding = np.repeat(x_date[-1:], num_padding, axis = 0)\n",
    "            y_date_padding = np.repeat(y_date[-1:], num_padding, axis = 0)\n",
    "            \n",
    "            xs = np.concatenate([xs, x_padding], axis=0)\n",
    "            ys = np.concatenate([ys, y_padding], axis=0)\n",
    "            \n",
    "            x_date = np.concatenate([x_date, x_date_padding], axis = 0)\n",
    "            y_date = np.concatenate([y_date, y_date_padding], axis = 0)\n",
    "            \n",
    "        self.size = len(xs)\n",
    "        self.num_batch = int(self.size // self.batch_size)\n",
    "        self.xs = xs\n",
    "        self.ys = ys\n",
    "        \n",
    "        self.x_date = x_date\n",
    "        self.y_date = y_date\n",
    "        \n",
    "        if shuffle:\n",
    "            permutation = np.random.permutation(self.size)\n",
    "            # print (\"permutation : \", len(permutation))\n",
    "            xs, ys = self.xs[permutation], self.ys[permutation]\n",
    "            # print (\"x_date len:\", x_date.shape)\n",
    "            # print (\"y_date len:\", y_date.shape)\n",
    "            \n",
    "            x_date, y_date = self.x_date[permutation], self.y_date[permutation]\n",
    "            self.xs = xs\n",
    "            self.ys = ys\n",
    "            self.x_date = x_date\n",
    "            self.y_date = y_date\n",
    "            \n",
    "    def get_iterator(self):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "\n",
    "        Yields:\n",
    "            (x_i, y_i, (x_date_i, y_date_i))\n",
    "        \"\"\"\n",
    "        self.current_ind = 0\n",
    "\n",
    "        def _wrapper():\n",
    "            while self.current_ind < self.num_batch:\n",
    "                start_ind = self.batch_size * self.current_ind\n",
    "                end_ind = min(self.size, self.batch_size * (self.current_ind + 1))\n",
    "                x_i = torch.Tensor(self.xs[start_ind: end_ind, ...]).to(self.device)\n",
    "                y_i = torch.Tensor(self.ys[start_ind: end_ind, ...]).to(self.device)\n",
    "                \n",
    "                x_date_i = self.x_date[start_ind: end_ind]\n",
    "                y_date_i = self.y_date[start_ind: end_ind]\n",
    "                yield (x_i, y_i, (x_date_i, y_date_i))\n",
    "                self.current_ind += 1\n",
    "\n",
    "        return _wrapper()\n",
    "    \n",
    "\n",
    "\n",
    "class DataLoaderSet:\n",
    "    \n",
    "    def __init__(self, X, Y, X_date, Y_date, path_ind_ls, train_ind, valid_ind_set, loader_params):\n",
    "        \"\"\" Building train/validation dataloader \n",
    "\n",
    "        Args:\n",
    "            path_ind_ls (tuple): set of path number corresponding to valid data set\n",
    "                ex\n",
    "                    (5, 2, 1)\n",
    "            train_ind (array): train index array set \n",
    "                ex\n",
    "                    array([1199, 1200, 1201, ..., 6295, 6296, 6297])\n",
    "            valid_ind_set (_type_): set of N number of validations index \n",
    "                ex\n",
    "                    [array([   0,    1,    2, ..., 1047, 1048, 1049]),\n",
    "                    array([2100, 2101, 2102, ..., 3147, 3148, 3149]),\n",
    "                    array([4200, 4201, 4202, ..., 5246, 5247, 5248])]\n",
    "            loader_params\n",
    "        Return \n",
    "            dataloader dictionary \n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.Y = Y \n",
    "        self.X_date = X_date\n",
    "        self.Y_date = Y_date\n",
    "        \n",
    "        self.path_ind_ls = path_ind_ls\n",
    "        self.train_ind = train_ind\n",
    "        self.valid_ind_set = valid_ind_set\n",
    "        self.loader_params = loader_params\n",
    "\n",
    "        self.dataloader = {}\n",
    "        self._trainloader()\n",
    "        self._validloader()\n",
    "\n",
    "    \n",
    "    def _trainloader(self):\n",
    "\n",
    "        # trainig\n",
    "        self.train_X, self.train_Y = self.X[self.train_ind], self.Y[self.train_ind]\n",
    "        self.train_x_date, self.train_y_date = self.X_date[self.train_ind], self.Y_date[self.train_ind]\n",
    "        \n",
    "        # scaling \n",
    "        scaling_value = self.train_X.reshape(-1, self.train_X.shape[-1])\n",
    "        mean, std = scaling_value.mean(axis = 0), scaling_value.std(axis = 0)\n",
    "        self.scaler = util.StandardScaler(mean, std)\n",
    "        \n",
    "        self.train_X = self.scaler.transform(self.train_X)\n",
    "        # TRAIN Dataloader generation \n",
    "        # print (\"Train loader generation\")\n",
    "        # train_loader = get_dataloader(train_X, train_Y, batch_size=batch_num, shuffle = True)\n",
    "        train_loader = CustomDataLoader(self.train_X, self.train_Y, self.train_x_date, self.train_y_date, self.loader_params['batch_size'], self.loader_params['device'], self.loader_params['shuffle'])\n",
    "        self.dataloader['train_loader'] = train_loader \n",
    "\n",
    "    def _validloader(self):\n",
    "\n",
    "        valid_loader_set = {} # key: path / value: valid loader\n",
    "\n",
    "        for ix, valid_ind in enumerate(self.valid_ind_set):\n",
    "            valid_X, valid_Y = self.X[valid_ind], self.Y[valid_ind]\n",
    "            x_date, y_date = self.X_date[valid_ind], self.Y_date[valid_ind]\n",
    "            valid_X = self.scaler.transform(valid_X)\n",
    "            # Custom Validation Loader \n",
    "            valid_loader = CustomDataLoader(valid_X, valid_Y, x_date, y_date, self.loader_params['batch_size'],  self.loader_params['device'], self.loader_params['shuffle'])\n",
    "            path_num = path_ind_ls[ix] # path number for each validation set \n",
    "            valid_loader_set[path_num] = valid_loader\n",
    "\n",
    "        self.dataloader['valid_loader'] = valid_loader_set\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_params = {'batch_size': 32,\n",
    "                 'shuffle':True,\n",
    "                 'device':'cuda:0'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1),\n",
       " (0, 2),\n",
       " (0, 3),\n",
       " (0, 4),\n",
       " (0, 5),\n",
       " (1, 2),\n",
       " (1, 3),\n",
       " (1, 4),\n",
       " (1, 5),\n",
       " (2, 3),\n",
       " (2, 4),\n",
       " (2, 5),\n",
       " (3, 4),\n",
       " (3, 5),\n",
       " (4, 5)]"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_ind_ls, train_ind, valid_ind_set = next(cv_split)\n",
    "path_ind_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2162, 2163, 2164, ..., 6295, 6296, 6297])"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4136"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([   0,    1,    2, ..., 1047, 1048, 1049]),\n",
       " array([1050, 1051, 1052, ..., 2097, 2098, 2099])]"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_ind_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0\n",
      "** Model Loading **\n",
      "DataLoader generation...\n",
      "Training...\n",
      "Validation...\n",
      "i: 1\n",
      "** Model Loading **\n",
      "DataLoader generation...\n",
      "Training...\n",
      "Validation...\n",
      "i: 2\n",
      "** Model Loading **\n",
      "DataLoader generation...\n",
      "Training...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18876/2297496742.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[0mloss_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m             \u001b[0mloss_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\etf\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\etf\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "total_loss = [0 for _ in range(path_num)]\n",
    "\n",
    "for idx, (path_ind_ls, train_ind, valid_ind_set) in enumerate(cv_split): # 결국 순서대로 -> 순서 자체가 train model number\n",
    "    \n",
    "    print ('i:', idx)\n",
    "    # MODEL INIT\n",
    "    ######### Model 을 정의하는 부분 ########\n",
    "    print (\"** Model Loading **\")\n",
    "    Model = LSTM_Model(num_assets, past_seq, future_seq, num_layers, device)\n",
    "\n",
    "    Model.to(device)\n",
    "\n",
    "    criterion = nn.L1Loss() # Loss function 을 정의합니다 : MAE\n",
    "    optimizer = torch.optim.Adam(Model.parameters(), lr = 0.01) # optimizer 을 설정해 줍니다. \n",
    "    \n",
    "    # DataLoader \n",
    "    print (\"DataLoader generation...\")\n",
    "    LoaderGenerator = DataLoaderSet(X, Y, X_date, Y_date, path_ind_ls, train_ind, valid_ind_set, loader_params)\n",
    "    loader_set = LoaderGenerator.dataloader\n",
    "    \n",
    "    # scaler2 for prediction during training\n",
    "    mean2 = torch.from_numpy(LoaderGenerator.scaler.mean).to(device)\n",
    "    std2 = torch.from_numpy(LoaderGenerator.scaler.std).to(device)\n",
    "\n",
    "    scaler2 = util.StandardScaler(mean2, std2)\n",
    "    # TRAIN     \n",
    "    # Epoch \n",
    "    print (\"Training...\")\n",
    "    for epoch in range(epoch_num):\n",
    "        Model.train()\n",
    "        epoch_train_loss = []\n",
    "        \n",
    "        train_loader = loader_set['train_loader']\n",
    "        for train_x, train_y, dates in train_loader.get_iterator():\n",
    "            \n",
    "            train_pred = Model(train_x)\n",
    "            \n",
    "            train_pred = scaler2.inverse_transform(train_pred)\n",
    "            \n",
    "            loss_train = criterion(train_pred, train_y)\n",
    "            loss_train.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "    # Validation \n",
    "    print (\"Validation...\")\n",
    "    Model.eval()\n",
    "    valid_loader_set = loader_set['valid_loader']\n",
    "    \n",
    "    for path_num in list(valid_loader_set.keys()):\n",
    "        # print (\"Path num: \", path_num)\n",
    "        n_valid_loader = valid_loader_set[path_num]\n",
    "        for valid_x, valid_y, dates in n_valid_loader.get_iterator():\n",
    "            val_pred = Model(valid_x)\n",
    "            val_pred = scaler2.inverse_transform(val_pred)\n",
    "            \n",
    "            val_loss = criterion(val_pred, valid_y)\n",
    "            \n",
    "            total_loss[path_num] += val_loss\n",
    "            \n",
    "for i in range(len(total_loss)):\n",
    "    total_loss[i] /= total_split_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns \n",
    "\n",
    "plt.title(\"Loss by Path\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Path Number\")\n",
    "plt.bar([i+1 for i in range(len(total_loss))] ,[np.array(x.detach().cpu()) for x in total_loss])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "26b462196a458278e1b93f4dafd0c6a4f17b941fc39fbc23177d70dd3e3b7653"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('tft')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
